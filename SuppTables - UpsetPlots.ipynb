{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# get the bucket name\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# Get the BigQuery curated dataset for the current workspace context.\n",
    "CDR = os.environ['WORKSPACE_CDR']\n",
    "\n",
    "from google.cloud import bigquery\n",
    "# Instantiate a BigQuery client\n",
    "client = bigquery.Client()\n",
    "#!pip install upsetplot #if necessary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', 7000)\n",
    "pd.set_option('display.max_row', 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318d91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85077d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTable(table_name, folder):\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file from the bucket to the current working space\n",
    "    os.system(f\"gsutil cp '{my_bucket}/data/{folder}/{table_name}' .\")\n",
    "\n",
    "    print(f'[INFO] {table_name} is successfully downloaded into your working space')\n",
    "    # save dataframe in a csv file in the same workspace as the notebook\n",
    "    table_read = pd.read_csv(table_name, sep=\"\\t\")\n",
    "    return table_read\n",
    "def getFile(table_name, folder):\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file from the bucket to the current working space\n",
    "    os.system(f\"gsutil cp '{my_bucket}/data/{folder}/{table_name}' .\")\n",
    "\n",
    "    print(f'[INFO] {table_name} is successfully downloaded into your working space')\n",
    "def describe_tables(**tables):\n",
    "    # Initialize a dictionary to store results\n",
    "    descriptions = {}\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    # Loop through each table with its name\n",
    "    for table_name, table in tables.items():\n",
    "        \n",
    "        table_unique = table.drop_duplicates(\"person_id\", keep = \"first\")\n",
    "        \n",
    "        desc = table_unique[\"AgeAtVisit_Years\"].describe()\n",
    "\n",
    "        # Calculate male and female counts and mf_ratio\n",
    "        male_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Male\", na=False)].shape[0]\n",
    "        female_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Female\", na=False)].shape[0]\n",
    "        mf_ratio = male_count / (male_count + female_count) if (male_count + female_count) > 0 else None\n",
    "\n",
    "        # Add the calculated values to the description as new rows\n",
    "        desc[\"male_count\"] = male_count\n",
    "        desc[\"female_count\"] = female_count\n",
    "        desc[\"mf_ratio\"] = mf_ratio\n",
    "        \n",
    "        #calculate solution annotated rate\n",
    "        \n",
    "\n",
    "        # Store the updated description in the dictionary\n",
    "        descriptions[table_name] = desc\n",
    "        i+=1\n",
    "\n",
    "    # Convert the dictionary of descriptions to a DataFrame\n",
    "    result_df = pd.DataFrame(descriptions).T  # Transpose to have table names as rows\n",
    "    result_df.index.name = \"Table_Name\"  # Set a meaningful index name\n",
    "    result_df = result_df.reset_index()\n",
    "\n",
    "    return result_df\n",
    "def annoConsentDate(table):\n",
    "    #annotate with consent dates\n",
    "\n",
    "    # Get the list of person IDs from the first query.\n",
    "    person_ids = table['person_id'].unique().tolist()\n",
    "    person_ids_query = ','.join(map(str, person_ids))\n",
    "\n",
    "    # Run the consent query using the person IDs from the first query.\n",
    "    consent_query = f'''\n",
    "    SELECT DISTINCT person_id, MAX(observation_date) AS primary_consent_date\n",
    "    FROM `{CDR}.concept`\n",
    "    JOIN `{CDR}.concept_ancestor` ON concept_id = ancestor_concept_id\n",
    "    JOIN `{CDR}.observation` ON descendant_concept_id = observation_source_concept_id\n",
    "    WHERE concept_name = 'Consent PII' \n",
    "      AND concept_class_id = 'Module'\n",
    "      AND person_id IN ({person_ids_query})\n",
    "    GROUP BY person_id\n",
    "    '''\n",
    "    consent_dates = pd.read_gbq(consent_query, progress_bar_type=\"tqdm_notebook\")\n",
    "\n",
    "    # Merge the results on person_id to incorporate the consent date.\n",
    "    final_result = table.merge(consent_dates, on='person_id', how='left')\n",
    "    \n",
    "    return final_result\n",
    "def describe_solution_tables(**tables):\n",
    "    # Initialize a dictionary to store results\n",
    "    descriptions = {}\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    # Loop through each table with its name\n",
    "    for table_name, table in tables.items():\n",
    "        \n",
    "        table_unique = table.drop_duplicates(\"person_id\", keep = \"first\")\n",
    "        \n",
    "        #table_unique = calcAgeAtConsent(table_unique)\n",
    "        \n",
    "        desc = table_unique[\"AgeAtConsent_Years\"].describe()\n",
    "\n",
    "        # Calculate male and female counts and mf_ratio\n",
    "        male_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Male\", na=False)].shape[0]\n",
    "        female_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Female\", na=False)].shape[0]\n",
    "        mf_ratio = male_count / (male_count + female_count) if (male_count + female_count) > 0 else None\n",
    "\n",
    "        # Add the calculated values to the description as new rows\n",
    "        desc[\"male_count\"] = male_count\n",
    "        desc[\"female_count\"] = female_count\n",
    "        desc[\"mf_ratio\"] = mf_ratio\n",
    "        \n",
    "        #calculate solution annotated rate\n",
    "        \n",
    "\n",
    "        # Store the updated description in the dictionary\n",
    "        descriptions[table_name] = desc\n",
    "        i+=1\n",
    "\n",
    "    # Convert the dictionary of descriptions to a DataFrame\n",
    "    result_df = pd.DataFrame(descriptions).T  # Transpose to have table names as rows\n",
    "    result_df.index.name = \"Table_Name\"  # Set a meaningful index name\n",
    "    result_df = result_df.reset_index()\n",
    "\n",
    "    return result_df\n",
    "def calcAgeToday(solution_person_table):\n",
    "    # Create a copy of the input DataFrame to avoid modifying the original\n",
    "    solution_person_table = solution_person_table.copy()\n",
    "\n",
    "    # Ensure 'birth_datetime' is in datetime format and make it timezone-naive\n",
    "    solution_person_table[\"birth_datetime\"] = pd.to_datetime(\n",
    "        solution_person_table[\"birth_datetime\"], errors=\"coerce\"\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    # Check for invalid or missing dates\n",
    "    if solution_person_table[\"birth_datetime\"].isnull().any():\n",
    "        print(\"Warning: Missing or invalid birth_datetime values detected.\")\n",
    "        print(solution_person_table[solution_person_table[\"birth_datetime\"].isnull()])\n",
    "\n",
    "    # Drop rows with invalid or missing 'birth_datetime'\n",
    "    solution_person_table = solution_person_table.dropna(subset=[\"birth_datetime\"])\n",
    "\n",
    "    # Get today's date as timezone-naive\n",
    "    today = datetime.now()\n",
    "\n",
    "    # Calculate the difference (result is a timedelta Series)\n",
    "    solution_person_table[\"AgeToday\"] = today - solution_person_table[\"birth_datetime\"]\n",
    "\n",
    "    # Ensure 'AgeToday' is a timedelta type\n",
    "    if not pd.api.types.is_timedelta64_dtype(solution_person_table[\"AgeToday\"]):\n",
    "        raise ValueError(\"AgeToday is not a valid timedelta64 dtype.\")\n",
    "\n",
    "    # Convert the timedelta to years\n",
    "    solution_person_table[\"AgeToday_Years\"] = (\n",
    "        solution_person_table[\"AgeToday\"].dt.days / 365.25\n",
    "    )\n",
    "\n",
    "    return solution_person_table\n",
    "def calcAgeAtConsent(ICD_person_table):\n",
    "    # Calculate the difference (result is a timedelta Series)\n",
    "    ICD_person_table[\"AgeAtConsent\"] = (\n",
    "        ICD_person_table[\"primary_consent_date\"] - ICD_person_table[\"birth_datetime\"]\n",
    "    )\n",
    "\n",
    "    # Convert the timedelta to years\n",
    "    ICD_person_table[\"AgeAtConsent_Years\"] = (\n",
    "        ICD_person_table[\"AgeAtConsent\"].dt.days / 365.25\n",
    "    )\n",
    "    \n",
    "    return ICD_person_table\n",
    "def calc_solved_rate(summary_df2):\n",
    "\n",
    "    concept_solved_n = summary_df2[~summary_df2['Table_Name'].str.contains(\"concept|female\", case=False)][\"count\"].sum()\n",
    "    summary_df2 = summary_df2.reset_index()\n",
    "    #print(summary_df2)\n",
    "    #print(concept_solved_n)\n",
    "\n",
    "    concept_n = summary_df2.loc[0,\"count\"]\n",
    "    #print(concept_n)\n",
    "    concept_solved_rate = concept_solved_n / concept_n\n",
    "\n",
    "    summary_df2.loc[0,\"solved_rate\"] = concept_solved_rate\n",
    "\n",
    "    subset_names = [\"AD\", \"XLmale\", \"XLfemale\", \"AR\", \"ADAR\"]\n",
    "    k=1\n",
    "    for i in subset_names:\n",
    "        i_reg = f\"^{i}_\"\n",
    "        i_tablename = f\"{i}_solutions\"\n",
    "\n",
    "        #print(f\"Table Name: {i_tablename}\")\n",
    "\n",
    "        # Access and print the DataFrame dynamically\n",
    "        if i_tablename in globals():\n",
    "            tbl = globals()[i_tablename]\n",
    "            tot_sol = tbl[\"person_id\"].nunique()\n",
    "            sol_anno = summary_df2[summary_df2[\"Table_Name\"].str.contains(i_reg, case=False)][\"count\"].sum()\n",
    "            sol_anno_rate = sol_anno / tot_sol\n",
    "            summary_df2.loc[k,\"solved_rate\"] = sol_anno_rate\n",
    "        else:\n",
    "            print(f\"{i_tablename} does not exist.\")\n",
    "        k+=1\n",
    "\n",
    "    return summary_df2 \n",
    "def calc_solved_rate_within(summary_df2):\n",
    "\n",
    "    concept_solved_n = summary_df2[~summary_df2['Table_Name'].str.contains(\"concept|female\", case=False)][\"count\"].sum()\n",
    "    summary_df2 = summary_df2.reset_index()\n",
    "    #print(summary_df2)\n",
    "    #print(concept_solved_n)\n",
    "\n",
    "    concept_n = summary_df2.loc[0,\"count\"]\n",
    "    #print(concept_n)\n",
    "    concept_solved_rate = concept_solved_n / concept_n\n",
    "\n",
    "    summary_df2.loc[0,\"solved_rate\"] = concept_solved_rate\n",
    "\n",
    "    subset_names = [\"AD\", \"XLmale\", \"XLfemale\", \"AR\", \"ADAR\"]\n",
    "    k=1\n",
    "    for i in subset_names:\n",
    "        i_reg = f\"^{i}_\"\n",
    "        i_tablename = f\"{i}_solutions\"\n",
    "\n",
    "        #print(f\"Table Name: {i_tablename}\")\n",
    "\n",
    "        # Access and print the DataFrame dynamically\n",
    "        if i_tablename in globals():\n",
    "            tbl = globals()[i_tablename]\n",
    "            tot_sol = tbl[\"person_id\"].nunique()\n",
    "            sol_anno = summary_df2[summary_df2[\"Table_Name\"].str.contains(i_reg, case=False)][\"count\"].sum()\n",
    "            sol_anno_rate = sol_anno / concept_n\n",
    "            summary_df2.loc[k,\"solved_rate\"] = sol_anno_rate\n",
    "        else:\n",
    "            print(f\"{i_tablename} does not exist.\")\n",
    "        k+=1\n",
    "\n",
    "    return summary_df2 \n",
    "def generate_merges(base_name, concept_person_table, solution_dict):\n",
    "    \"\"\"\n",
    "    Dynamically generate merged tables, add them to the global namespace, and return a list of table names.\n",
    "\n",
    "    Parameters:\n",
    "    - base_name (str): The base name to replace \"retdegen\" in the generated table names.\n",
    "    - concept_person_table (pd.DataFrame): The concept-person DataFrame to merge on.\n",
    "    - solution_dict (dict): A dictionary where keys are solution table names (e.g., \"AD_solutions\")\n",
    "                            and values are the corresponding DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of dynamically generated table names.\n",
    "    \"\"\"\n",
    "    # Initialize the list to store table names\n",
    "    table_names = []\n",
    "    \n",
    "    # Remove conflicting global variables\n",
    "    #for key in list(globals().keys()):\n",
    "    #    if key.endswith(\"_in_retdegen\") or key == \"retdegen_concept_person\":\n",
    "    #        print(f\"Removing stale variable: {key}\")\n",
    "    #        del globals()[key]\n",
    "\n",
    "    for solution_name, solution_table in solution_dict.items():\n",
    "        # Construct the target table name dynamically\n",
    "        target_table_name = f\"{solution_name.split('_')[0]}_in_{base_name}\"\n",
    "        table_names.append(target_table_name)  # Add the table name to the list\n",
    "        \n",
    "\n",
    "        # Perform the merge\n",
    "        merged_table = pd.merge(\n",
    "            solution_table[['variant_id', 'person_id', 'allele_count', 'GeneID_EG', 'GeneID_Symbol']],\n",
    "            concept_person_table,\n",
    "            on=\"person_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Assign the result to the global namespace\n",
    "        globals()[target_table_name] = merged_table\n",
    "        print(f\"Generated table: {target_table_name}\")\n",
    "\n",
    "    # Add the concept-person table to the list of table names\n",
    "    concept_table_name = f\"{base_name}_concept_person\"\n",
    "    table_names.insert(0, concept_table_name)  # Ensure it appears first in the list\n",
    "    globals()[concept_table_name] = concept_person_table  # Assign it to the global namespace\n",
    "    #print(f\"Generated table: {concept_table_name}\")\n",
    "\n",
    "    return table_names\n",
    "def process_concept_and_solution_tables(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concept and solution tables.\n",
    "    \"\"\"\n",
    "    all_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Describe the merged tables\n",
    "        summary_df = describe_tables(**merged_tables)\n",
    "\n",
    "        # Calculate solved rates\n",
    "        solved_summary = calc_solved_rate(summary_df)\n",
    "\n",
    "        # Add a column to identify the concept\n",
    "        solved_summary[\"concept\"] = concept_name\n",
    "\n",
    "        # Append the summary to the list\n",
    "        all_summaries.append(solved_summary)\n",
    "\n",
    "    # Combine all summaries into one big table\n",
    "    final_summary_table = pd.concat(all_summaries, axis=0)\n",
    "\n",
    "    return final_summary_table\n",
    "def process_concept_and_solution_tables_within(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concept and solution tables.\n",
    "    \"\"\"\n",
    "    all_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Describe the merged tables\n",
    "        summary_df = describe_tables(**merged_tables)\n",
    "\n",
    "        # Calculate solved rates\n",
    "        solved_summary = calc_solved_rate_within(summary_df)\n",
    "\n",
    "        # Add a column to identify the concept\n",
    "        solved_summary[\"concept\"] = concept_name\n",
    "\n",
    "        # Append the summary to the list\n",
    "        all_summaries.append(solved_summary)\n",
    "\n",
    "    # Combine all summaries into one big table\n",
    "    final_summary_table = pd.concat(all_summaries, axis=0)\n",
    "\n",
    "    return final_summary_table\n",
    "def process_concept_solution_gene_summaries(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates gene-level summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concepts, solutions, and genes.\n",
    "    \"\"\"\n",
    "    all_gene_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Loop through each merged table\n",
    "        for table_name, merged_table in merged_tables.items():\n",
    "            # Ensure 'GeneID_Symbol' exists in the merged table\n",
    "            if \"GeneID_Symbol\" not in merged_table.columns:\n",
    "                print(f\"Warning: 'GeneID_Symbol' column not found in {table_name}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Group by GeneID_Symbol and describe each group\n",
    "            gene_groups = merged_table.groupby(\"GeneID_Symbol\")\n",
    "\n",
    "\n",
    "            for gene, group in gene_groups:\n",
    "                # Generate summaries for the current gene\n",
    "                \n",
    "                # find all NaN entries in condition i.e. unannotated\n",
    "                nan_mask = group.condition_source_value.isnull() # or df.isna()\n",
    "                \n",
    "                summary = {\n",
    "                    \"concept\": concept_name,\n",
    "                    \"table\": table_name,\n",
    "                    \"gene\": gene,\n",
    "                    \"variant_count\":  group[~nan_mask].variant_id.nunique(),\n",
    "                    \"person_count\": group[~nan_mask].person_id.nunique()\n",
    "                    #\"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                    #\"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                    #\"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                    #\"allele_count_max\": group[\"allele_count\"].max(),\n",
    "                }\n",
    "                # Append the summary to the list\n",
    "                all_gene_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    gene_summary_table = pd.DataFrame(all_gene_summaries)\n",
    "    \n",
    "    gene_summary_table['solution_table'] = gene_summary_table['table'].str.split('_').str[0]\n",
    "\n",
    "\n",
    "    return gene_summary_table\n",
    "def process_solution_gene_summaries(solution_tables):\n",
    "    \"\"\"\n",
    "    Generates gene-level summaries directly from solution tables.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing gene-level summaries for all solution tables.\n",
    "    \"\"\"\n",
    "    all_gene_summaries = []\n",
    "\n",
    "    # Loop through each solution table\n",
    "    for table_name, solution_table in solution_tables.items():\n",
    "        # Ensure 'GeneID_Symbol' exists in the solution table\n",
    "        if \"GeneID_Symbol\" not in solution_table.columns:\n",
    "            print(f\"Warning: 'GeneID_Symbol' column not found in {table_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Group by GeneID_Symbol and describe each group\n",
    "        gene_groups = solution_table.groupby(\"GeneID_Symbol\")\n",
    "\n",
    "        for gene, group in gene_groups:\n",
    "            # Generate summaries for the current gene\n",
    "            summary = {\n",
    "                \"table\": table_name,\n",
    "                \"gene\": gene,\n",
    "                \"variant_count\": group[\"variant_id\"].nunique(),\n",
    "                \"person_count\": group[\"person_id\"].nunique(),\n",
    "                \"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                \"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                \"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                \"allele_count_max\": group[\"allele_count\"].max(),\n",
    "            }\n",
    "            # Append the summary to the list\n",
    "            all_gene_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    gene_summary_table = pd.DataFrame(all_gene_summaries)\n",
    "\n",
    "    return gene_summary_table\n",
    "def calculate_annotation_rates(solution_gene_summary, concept_gene_summary):\n",
    "    \"\"\"\n",
    "    Calculates annotation rates at the gene and variant levels by comparing solution summaries\n",
    "    and concept-intersection summaries.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_gene_summary (pd.DataFrame): Gene-level summaries for solution tables \n",
    "                                             (output from process_solution_gene_summaries).\n",
    "    - concept_gene_summary (pd.DataFrame): Gene-level summaries for intersection tables \n",
    "                                           (output from process_concept_solution_gene_summaries).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing annotation rates at the gene and variant levels.\n",
    "    \"\"\"\n",
    "    # Merge the solution and concept summaries on the common columns\n",
    "    merged = pd.merge(\n",
    "        concept_gene_summary,\n",
    "        solution_gene_summary,\n",
    "        on=[\"table\", \"gene\"],\n",
    "        suffixes=(\"_concept\", \"_solution\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Calculate annotation rates\n",
    "    merged[\"person_annotation_rate\"] = (\n",
    "        merged[\"person_count_concept\"] / merged[\"person_count_solution\"]\n",
    "    )\n",
    "    merged[\"variant_annotation_rate\"] = (\n",
    "        merged[\"variant_count_concept\"] / merged[\"variant_count_solution\"]\n",
    "    )\n",
    "\n",
    "    # Handle divisions by zero or missing values\n",
    "    merged[\"person_annotation_rate\"] = merged[\"person_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "    merged[\"variant_annotation_rate\"] = merged[\"variant_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "\n",
    "    # Return the annotated DataFrame\n",
    "    return merged\n",
    "def calculate_all_annotation_rates(solution_gene_summary, concept_gene_summary):\n",
    "    \"\"\"\n",
    "    Calculates annotation rates at the gene and variant levels for each category\n",
    "    by comparing solution and concept summaries.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_gene_summary (pd.DataFrame): Gene-level summaries for solution tables.\n",
    "    - concept_gene_summary (pd.DataFrame): Gene-level summaries for intersection tables.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing annotation rates at the gene and variant levels for each category.\n",
    "    \"\"\"\n",
    "    all_annotation_rates = []\n",
    "    \n",
    "    # Make necessary columns\n",
    "    concept_gene_summary[\"category\"] = concept_gene_summary[\"table\"].str.extract(r\"in_([a-zA-Z]+)\").fillna(\"\")\n",
    "    concept_gene_summary[\"category_inheritance\"] = concept_gene_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_in\").fillna(\"\")\n",
    "    solution_gene_summary[\"category\"] = solution_gene_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_solutions\").fillna(\"\")\n",
    "\n",
    "\n",
    "    # Ensure both summaries have the necessary columns\n",
    "    required_columns = {\"table\", \"gene\", \"person_count\", \"variant_count\", \"category\"}\n",
    "    if not required_columns.issubset(solution_gene_summary.columns):\n",
    "        raise ValueError(\"Solution gene summary is missing required columns.\")\n",
    "    if not required_columns.issubset(concept_gene_summary.columns):\n",
    "        raise ValueError(\"Concept gene summary is missing required columns.\")\n",
    "\n",
    "    # Process each category separately\n",
    "    for con_category in concept_gene_summary[\"category\"].unique():\n",
    "        # Filter concept summaries for the current category\n",
    "        concept_subset = concept_gene_summary[concept_gene_summary[\"category\"] == con_category]\n",
    "        \n",
    "        for sol_category in solution_gene_summary[\"category\"].unique():\n",
    "            # Filter solution summaries for the current category\n",
    "            sol_category_q = f\"^{sol_category}\"\n",
    "            consol_category_q = f\"^{sol_category}$\"\n",
    "            solution_subset = solution_gene_summary[solution_gene_summary[\"category\"].str.contains(sol_category_q)]\n",
    "            concept_subset_inh = concept_subset[concept_subset[\"category_inheritance\"].str.contains(consol_category_q)]\n",
    "\n",
    "            if concept_subset_inh.empty or solution_subset.empty:\n",
    "                print(f\"Skipping category '{con_category}' / '{sol_category}' because one of the subsets is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Debugging: Check subset details\n",
    "            print(f\"Processing category '{con_category}' / '{sol_category}'\")\n",
    "            #print(\"Concept Subset:\")\n",
    "            #print(concept_subset_inh.head())\n",
    "\n",
    "            # Merge the summaries for this category\n",
    "            merged = pd.merge(\n",
    "                concept_subset_inh,\n",
    "                solution_subset[['variant_count', 'person_count', 'gene']],\n",
    "                on=[\"gene\"],\n",
    "                suffixes=(\"_concept\", \"_solution\"),\n",
    "                how=\"inner\"\n",
    "            )\n",
    "\n",
    "            # Calculate annotation rates\n",
    "            merged[\"person_annotation_rate\"] = (\n",
    "                merged[\"person_count_concept\"] / merged[\"person_count_solution\"]\n",
    "            )\n",
    "            merged[\"variant_annotation_rate\"] = (\n",
    "                merged[\"variant_count_concept\"] / merged[\"variant_count_solution\"]\n",
    "            )\n",
    "\n",
    "            # Handle divisions by zero or missing values\n",
    "            merged[\"person_annotation_rate\"] = merged[\"person_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "            merged[\"variant_annotation_rate\"] = merged[\"variant_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "\n",
    "            # Append to the results list\n",
    "            all_annotation_rates.append(merged)\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    if all_annotation_rates:\n",
    "        final_annotation_rates = pd.concat(all_annotation_rates, axis=0, ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        final_annotation_rates = pd.DataFrame()  # Return an empty DataFrame if no data\n",
    "    \n",
    "    return final_annotation_rates\n",
    "def process_solution_variant_summaries(solution_tables):\n",
    "    \"\"\"\n",
    "    Generates variant-level summaries directly from solution tables.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing variant-level summaries for all solution tables.\n",
    "    \"\"\"\n",
    "    all_variant_summaries = []\n",
    "\n",
    "    # Loop through each solution table\n",
    "    for table_name, solution_table in solution_tables.items():\n",
    "        # Ensure 'variant_id' exists in the solution table\n",
    "        if \"variant_id\" not in solution_table.columns:\n",
    "            print(f\"Warning: 'variant_id' column not found in {table_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Group by variant_id and describe each group\n",
    "        variant_groups = solution_table.groupby(\"variant_id\")\n",
    "\n",
    "        for variant, group in variant_groups:\n",
    "            # Generate summaries for the current variant\n",
    "            summary = {\n",
    "                \"table\": table_name,\n",
    "                \"variant_id\": variant,\n",
    "                \"gene\": group[\"GeneID_Symbol\"].iloc[0] if \"GeneID_Symbol\" in group.columns else None,\n",
    "                \"person_count\": group[\"person_id\"].nunique(),\n",
    "                \"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                \"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                \"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                \"allele_count_max\": group[\"allele_count\"].max(),\n",
    "            }\n",
    "            # Append the summary to the list\n",
    "            all_variant_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    variant_summary_table = pd.DataFrame(all_variant_summaries)\n",
    "\n",
    "    return variant_summary_table\n",
    "def process_concept_solution_variant_summaries(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates variant-level summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concepts, solutions, and variants.\n",
    "    \"\"\"\n",
    "    all_variant_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Loop through each merged table\n",
    "        for table_name, merged_table in merged_tables.items():\n",
    "            # Ensure 'variant_id' exists in the merged table\n",
    "            if \"variant_id\" not in merged_table.columns:\n",
    "                print(f\"Warning: 'variant_id' column not found in {table_name}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Group by variant_id and describe each group\n",
    "            variant_groups = merged_table.groupby(\"variant_id\")\n",
    "\n",
    "            for variant, group in variant_groups:\n",
    "                # Generate summaries for the current variant\n",
    "                \n",
    "                # find all NaN entries in condition i.e. unannotated\n",
    "                nan_mask = group.condition_source_value.isnull() # or df.isna()\n",
    "                \n",
    "                summary = {\n",
    "                    \"concept\": concept_name,\n",
    "                    \"table\": table_name,\n",
    "                    \"variant_id\": variant,\n",
    "                    \"gene\": group[\"GeneID_Symbol\"].iloc[0] if \"GeneID_Symbol\" in group.columns else None,\n",
    "                    \"person_count\": group[~nan_mask].person_id.nunique()\n",
    "                    #\"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                    #\"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                    #\"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                    #\"allele_count_max\": group[\"allele_count\"].max(),\n",
    "                }\n",
    "                # Append the summary to the list\n",
    "                all_variant_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    variant_summary_table = pd.DataFrame(all_variant_summaries)\n",
    "\n",
    "    return variant_summary_table\n",
    "def calculate_all_variant_annotation_rates(solution_variant_summary, concept_variant_summary):\n",
    "    \"\"\"\n",
    "    Calculates annotation rates at the variant level for each category\n",
    "    by comparing solution and concept summaries.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_variant_summary (pd.DataFrame): Variant-level summaries for solution tables.\n",
    "    - concept_variant_summary (pd.DataFrame): Variant-level summaries for intersection tables.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing annotation rates at the variant level for each category.\n",
    "    \"\"\"\n",
    "    all_annotation_rates = []\n",
    "\n",
    "    # Add necessary columns for categorization\n",
    "    concept_variant_summary[\"category\"] = concept_variant_summary[\"table\"].str.extract(r\"in_([a-zA-Z]+)\").fillna(\"\")\n",
    "    concept_variant_summary[\"category_inheritance\"] = concept_variant_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_in\").fillna(\"\")\n",
    "    solution_variant_summary[\"category\"] = solution_variant_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_solutions\").fillna(\"\")\n",
    "\n",
    "    # Ensure both summaries have the required columns\n",
    "    required_columns = {\"table\", \"variant_id\", \"person_count\", \"allele_count_mean\", \"category\"}\n",
    "    if not required_columns.issubset(solution_variant_summary.columns):\n",
    "        raise ValueError(\"Solution variant summary is missing required columns.\")\n",
    "    if not required_columns.issubset(concept_variant_summary.columns):\n",
    "        raise ValueError(\"Concept variant summary is missing required columns.\")\n",
    "\n",
    "    # Process each category separately\n",
    "    for con_category in concept_variant_summary[\"category\"].unique():\n",
    "        # Filter concept summaries for the current category\n",
    "        concept_subset = concept_variant_summary[concept_variant_summary[\"category\"] == con_category]\n",
    "\n",
    "        for sol_category in solution_variant_summary[\"category\"].unique():\n",
    "            # Filter solution summaries for the current category\n",
    "            sol_category_q = f\"^{sol_category}\"\n",
    "            consol_category_q = f\"^{sol_category}$\"\n",
    "            solution_subset = solution_variant_summary[solution_variant_summary[\"category\"].str.contains(sol_category_q)]\n",
    "            concept_subset_inh = concept_subset[concept_subset[\"category_inheritance\"].str.contains(consol_category_q)]\n",
    "\n",
    "            if concept_subset_inh.empty or solution_subset.empty:\n",
    "                print(f\"Skipping category '{con_category}' / '{sol_category}' because one of the subsets is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Debugging: Check subset details\n",
    "            print(f\"Processing category '{con_category}' / '{sol_category}'\")\n",
    "\n",
    "            # Merge the summaries for this category\n",
    "            merged = pd.merge(\n",
    "                concept_subset_inh,\n",
    "                solution_subset[['variant_id', 'person_count', 'allele_count_mean']],\n",
    "                on=[\"variant_id\"],\n",
    "                suffixes=(\"_concept\", \"_solution\"),\n",
    "                how=\"inner\"\n",
    "            )\n",
    "\n",
    "            # Calculate annotation rates\n",
    "            merged[\"person_annotation_rate\"] = (\n",
    "                merged[\"person_count_concept\"] / merged[\"person_count_solution\"]\n",
    "            )\n",
    "            merged[\"allele_annotation_rate\"] = (\n",
    "                merged[\"allele_count_mean_concept\"] / merged[\"allele_count_mean_solution\"]\n",
    "            )\n",
    "\n",
    "            # Handle divisions by zero or missing values\n",
    "            merged[\"person_annotation_rate\"] = merged[\"person_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "            merged[\"allele_annotation_rate\"] = merged[\"allele_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "\n",
    "            # Append to the results list\n",
    "            all_annotation_rates.append(merged)\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    if all_annotation_rates:\n",
    "        final_annotation_rates = pd.concat(all_annotation_rates, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        final_annotation_rates = pd.DataFrame()  # Return an empty DataFrame if no data\n",
    "\n",
    "    return final_annotation_rates\n",
    "def saveToBucket(df, df_filename, data_folder):\n",
    "    df.to_csv(df_filename, sep = \"\\t\", index=False)\n",
    "\n",
    "    # get the bucket name\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file to the bucket\n",
    "    args = [\"gsutil\", \"cp\", f\"./{df_filename}\", f\"{my_bucket}/data/{data_folder}/\"]\n",
    "    output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "    # print output from gsutil\n",
    "    output.stderr\n",
    "def concatenate_variant_columns(df, chrom_col='CHROM', pos_col='POS', ref_col='REF', alt_col='ALT', new_col='Variant'):\n",
    "    \"\"\"\n",
    "    Concatenates CHROM, POS, REF, and ALT columns into a single Variant column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the columns to concatenate.\n",
    "    - chrom_col (str): Name of the chromosome column. Default is 'CHROM'.\n",
    "    - pos_col (str): Name of the position column. Default is 'POS'.\n",
    "    - ref_col (str): Name of the reference allele column. Default is 'REF'.\n",
    "    - alt_col (str): Name of the alternate allele column. Default is 'ALT'.\n",
    "    - new_col (str): Name of the new concatenated column. Default is 'Variant'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new concatenated column.\n",
    "    \"\"\"\n",
    "    # Ensure the required columns exist\n",
    "    required_cols = [chrom_col, pos_col, ref_col, alt_col]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n",
    "\n",
    "    # Convert all columns to string to ensure proper concatenation\n",
    "    df[new_col] = df[chrom_col].astype(str) + '-' + \\\n",
    "                 df[pos_col].astype(str) + '-' + \\\n",
    "                 df[ref_col].astype(str) + '-' + \\\n",
    "                 df[alt_col].astype(str)\n",
    "\n",
    "    return df\n",
    "def split_variant_column(df, variant_col='Variant', chrom_col='CHROM', pos_col='POS', ref_col='REF', alt_col='ALT', sep='-'):\n",
    "    \"\"\"\n",
    "    Splits a Variant column into CHROM, POS, REF, and ALT columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the Variant column.\n",
    "    - variant_col (str): Name of the concatenated Variant column. Default is 'Variant'.\n",
    "    - chrom_col (str): Name of the chromosome column to create. Default is 'CHROM'.\n",
    "    - pos_col (str): Name of the position column to create. Default is 'POS'.\n",
    "    - ref_col (str): Name of the reference allele column to create. Default is 'REF'.\n",
    "    - alt_col (str): Name of the alternate allele column to create. Default is 'ALT'.\n",
    "    - sep (str): Separator used in the Variant column. Default is '-'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the split columns.\n",
    "    \"\"\"\n",
    "    if variant_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{variant_col}' not found in the DataFrame.\")\n",
    "\n",
    "    # Split the Variant column into multiple columns\n",
    "    split_cols = df[variant_col].str.split(sep, expand=True)\n",
    "\n",
    "    if split_cols.shape[1] != 4:\n",
    "        raise ValueError(f\"Expected 4 components in the Variant column separated by '{sep}', but got {split_cols.shape[1]}.\")\n",
    "\n",
    "    # Assign the split columns to respective new columns\n",
    "    df[chrom_col] = split_cols[0]\n",
    "    df[pos_col] = split_cols[1]\n",
    "    df[ref_col] = split_cols[2]\n",
    "    df[alt_col] = split_cols[3]\n",
    "\n",
    "    return df\n",
    "def annotate_variants(\n",
    "    anno_vars_table: pd.DataFrame,\n",
    "    clinvar_query_table: pd.DataFrame,\n",
    "    VAT_query_table: pd.DataFrame,\n",
    "    remove_list: list = None,\n",
    "    filter_canonical: bool = True,\n",
    "    drop_original_variant_columns: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Annotate a table of genetic variants with ClinVar and VAT annotations.\n",
    "\n",
    "    Parameters:\n",
    "    - anno_vars_table (pd.DataFrame): DataFrame containing variants to annotate. Must include 'variant_id'.\n",
    "    - clinvar_query_table (pd.DataFrame): DataFrame containing ClinVar annotations.\n",
    "    - VAT_query_table (pd.DataFrame): DataFrame containing VAT annotations.\n",
    "    - remove_list (list, optional): List of transcript consequences to exclude. Defaults to None.\n",
    "    - filter_canonical (bool, optional): If True, filter VAT to canonical transcripts. Defaults to True.\n",
    "    - drop_original_variant_columns (bool, optional): If True, drop 'VARIANT_forQuery' and 'vid' after merging. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Annotated DataFrame with ClinVar and VAT information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input DataFrames\n",
    "    required_clinvar_cols = {'VARIANT_forQuery', 'CLNHGVS', 'CLNREVSTAT', \n",
    "                             'CLNSIG', 'MC', 'Type', 'Name', 'RCVaccession'}\n",
    "    if not required_clinvar_cols.issubset(clinvar_query_table.columns):\n",
    "        missing = required_clinvar_cols - set(clinvar_query_table.columns)\n",
    "        raise ValueError(f\"ClinVar query table is missing columns: {missing}\")\n",
    "    \n",
    "    required_VAT_cols = {'vid', 'gvs_all_af', 'transcript_source', 'transcript', \n",
    "                        'dna_change_in_transcript', 'aa_change', 'consequence',\n",
    "                        'variant_type', 'exon_number', 'intron_number', 'gene_id',\n",
    "                        'revel', 'splice_ai_acceptor_gain_score', \n",
    "                        'splice_ai_acceptor_loss_score',\n",
    "                        'splice_ai_donor_gain_score', \n",
    "                        'splice_ai_donor_loss_score',\n",
    "                        'omim_phenotypes_id', 'omim_phenotypes_name',\n",
    "                        'clinvar_classification', 'entrezgene', 'is_canonical_transcript'}\n",
    "    if not required_VAT_cols.issubset(VAT_query_table.columns):\n",
    "        missing = required_VAT_cols - set(VAT_query_table.columns)\n",
    "        raise ValueError(f\"VAT query table is missing columns: {missing}\")\n",
    "    \n",
    "    # Validate anno_vars_table\n",
    "    if 'variant_id' not in anno_vars_table.columns:\n",
    "        raise ValueError(\"anno_vars_table must contain a 'variant_id' column.\")\n",
    "    \n",
    "    # Step 1: Merge with ClinVar Query Table\n",
    "    # Select necessary columns from ClinVar\n",
    "    clinvar_cols_to_merge = ['VARIANT_forQuery', 'CLNHGVS', 'CLNREVSTAT', 'ID',\n",
    "                             'CLNSIG', 'MC', 'Type', 'Name', 'RCVaccession']\n",
    "    \n",
    "    # Perform the merge\n",
    "    merged_df = anno_vars_table.merge(\n",
    "        clinvar_query_table[clinvar_cols_to_merge],\n",
    "        how='left',\n",
    "        left_on='variant_id',\n",
    "        right_on='VARIANT_forQuery'\n",
    "    )\n",
    "    \n",
    "    # Drop 'VARIANT_forQuery' if required\n",
    "    if drop_original_variant_columns:\n",
    "        merged_df.drop(columns=['VARIANT_forQuery'], inplace=True)\n",
    "    \n",
    "    # Step 2: Filter VAT Query Table\n",
    "    VAT_query_table_filtered = VAT_query_table.copy()\n",
    "    \n",
    "    if filter_canonical:\n",
    "        # Filter to canonical transcripts\n",
    "        if 'is_canonical_transcript' not in VAT_query_table_filtered.columns:\n",
    "            raise ValueError(\"VAT_query_table must contain 'is_canonical_transcript' column for filtering.\")\n",
    "        VAT_query_table_filtered = VAT_query_table_filtered[VAT_query_table_filtered[\"is_canonical_transcript\"] == True]\n",
    "    elif remove_list is not None:\n",
    "        # Remove specified transcript consequences\n",
    "        VAT_query_table_filtered = VAT_query_table_filtered[~VAT_query_table_filtered[\"consequence\"].isin(remove_list)]\n",
    "    \n",
    "    # Step 3: Merge with VAT Query Table\n",
    "    # Select necessary columns from VAT\n",
    "    VAT_cols_to_merge = [\n",
    "        'vid', 'gvs_all_af', 'transcript_source', 'transcript', \n",
    "        'dna_change_in_transcript', 'aa_change', 'consequence',\n",
    "        'variant_type', 'exon_number', 'intron_number', 'gene_id',\n",
    "        'revel', 'splice_ai_acceptor_gain_score', \n",
    "        'splice_ai_acceptor_loss_score',\n",
    "        'splice_ai_donor_gain_score', \n",
    "        'splice_ai_donor_loss_score',\n",
    "        'omim_phenotypes_id', 'omim_phenotypes_name',\n",
    "        'clinvar_classification', 'entrezgene'\n",
    "    ]\n",
    "    \n",
    "    merged_df_vat = merged_df.merge(\n",
    "        VAT_query_table_filtered[VAT_cols_to_merge],\n",
    "        how='left',\n",
    "        left_on='variant_id',\n",
    "        right_on='vid'\n",
    "    )\n",
    "    \n",
    "    # Drop 'vid' if required\n",
    "    if drop_original_variant_columns:\n",
    "        merged_df_vat.drop(columns=['vid'], inplace=True)\n",
    "    \n",
    "    # Optional: Handle multiple VAT annotations per variant_id\n",
    "    # Depending on the data structure, you might want to aggregate or handle duplicates\n",
    "    \n",
    "    return merged_df_vat\n",
    "def processConsentDate(tbl):\n",
    "    #gets consent date for each participant in any table with person_id and calculates age at consent\n",
    "    tbl = annoConsentDate(tbl)\n",
    "    tbl['birth_datetime'] = pd.to_datetime(tbl['birth_datetime'])\n",
    "    tbl['primary_consent_date'] = pd.to_datetime(tbl['primary_consent_date'])\n",
    "    tbl['primary_consent_date'] = tbl['primary_consent_date'].dt.tz_localize('UTC')\n",
    "    tbl = calcAgeAtConsent(tbl)\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8dc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get concept_person tables\n",
    "retdys_concept_person = getTable(table_name = \"retchordys_ICD_person_age_youngest.tsv\", folder = \"personID_concept\")\n",
    "retdegen_concept_person = getTable(table_name = \"retdegen_ICD_person_age_youngest.tsv\", folder = \"personID_concept\")\n",
    "screenretdys_concept_person = getTable(table_name = \"ScreenRetDysICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "AMDspecific_concept_person = getTable(table_name = \"AMD_specific_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "AMDexudative_concept_person = getTable(table_name = \"AMD_exudative_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "AMDnonexudative_concept_person = getTable(table_name = \"AMD_nonexudative_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "CME_concept_person = getTable(table_name = \"CME_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "myopia_concept_person = getTable(table_name = \"myopia_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "hypermetropia_concept_person = getTable(table_name = \"hypermetropia_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "pucker_concept_person = getTable(table_name = \"pucker_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "#get concept icd codes\n",
    "retchordys_ICD = getTable(\"retchordys_ICD.tsv\", \"personID_concept\")\n",
    "retdegen_ICD = getTable(table_name = \"retdegen_ICD.tsv\", folder = \"personID_concept\")\n",
    "screenretdys_ICD = getTable(table_name = \"ScreenRetDysICD.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "AMD_ICD = getTable(\"AMD_specific_ICD_codes.tsv\", \"Concept_Sets_ICDcodes\")\n",
    "CME_ICD = getTable(\"CME_ICD_codes.tsv\", \"Concept_Sets_ICDcodes\")\n",
    "\n",
    "\n",
    "pucker_ICD = getTable(\"pucker_ICD.tsv\", \"personID_concept\")\n",
    "myopia_ICD = getTable(\"myopia_ICD_codes.tsv\", \"Concept_Sets_ICDcodes\")\n",
    "hypermetropia_ICD = getTable(\"hypermetropia_ICD_codes.tsv\", \"Concept_Sets_ICDcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upset plots of ICD codes in each code set\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import from_contents\n",
    "\n",
    "concepts = from_contents(\n",
    "    {#\"ALLICD\": list(set(conceptids_list_ALLICD)), \n",
    "     \"IRD\": set(retchordys_ICD.ICD_conceptID.to_list()), \n",
    "     \"Retinopathy\": set(retdegen_ICD.ICD_conceptID.to_list()),\n",
    "     \"Screening Set\" : set(screenretdys_ICD.ICD_conceptID.to_list())\n",
    "     #\"AMD_All\": AMDspecific_concept_person_pid,\n",
    "     #\"AMD_Ex\": AMDexudative_concept_person_pid,\n",
    "     #\"AMD_NonEx\" : AMDnonexudative_concept_person_pid,\n",
    "     #\"CME\": CME_concept_person_pid,\n",
    "     #\"Pucker\": pucker_concept_person_pid,\n",
    "     #\"RSS\": concept_ids_list_Retinoschisis_ICD,\n",
    "     #\"Myopia\": myopia_concept_person_pid,\n",
    "     #\"Hypermetropia\": hypermetropia_concept_person_pid,\n",
    "    }\n",
    ")\n",
    "#concepts\n",
    "from upsetplot import UpSet\n",
    "ax_dict = UpSet(concepts, subset_size=\"count\", show_counts = True, min_subset_size=None).plot()\n",
    "plt.savefig(\"upsetplot_nested_sets_codes.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "#alter as necessary for desired upset plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ca159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upset plots of person_IDs in each code set\n",
    "\n",
    "\n",
    "from upsetplot import from_contents\n",
    "\n",
    "concepts = from_contents(\n",
    "    {#\"ALLICD\": list(set(conceptids_list_ALLICD)), \n",
    "     \"IRD\": retdys_concept_person_pid, \n",
    "     \"Retinopathy\": retdegen_concept_person_pid,\n",
    "     \"Screening Set\" : screenretdys_concept_person_pid\n",
    "     #\"AMD_All\": AMDspecific_concept_person_pid,\n",
    "     #\"AMD_Ex\": AMDexudative_concept_person_pid,\n",
    "     #\"AMD_NonEx\" : AMDnonexudative_concept_person_pid,\n",
    "     #\"CME\": CME_concept_person_pid,\n",
    "     #\"Pucker\": pucker_concept_person_pid,\n",
    "     #\"RSS\": concept_ids_list_Retinoschisis_ICD,\n",
    "     #\"Myopia\": myopia_concept_person_pid,\n",
    "     #\"Hypermetropia\": hypermetropia_concept_person_pid,\n",
    "    }\n",
    ")\n",
    "#concepts\n",
    "\n",
    "ax_dict = UpSet(concepts, subset_size=\"count\", show_counts = True, min_subset_size=None).plot()\n",
    "plt.savefig(\"upsetplot_nested_pID.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "#alter as necessary for desired upset plot"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
