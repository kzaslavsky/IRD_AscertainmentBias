{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb020842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# get the bucket name\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# Get the BigQuery curated dataset for the current workspace context.\n",
    "CDR = os.environ['WORKSPACE_CDR']\n",
    "\n",
    "from google.cloud import bigquery\n",
    "# Instantiate a BigQuery client\n",
    "client = bigquery.Client()\n",
    "#!pip install upsetplot #if necessary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', 7000)\n",
    "pd.set_option('display.max_row', 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTable(table_name, folder):\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file from the bucket to the current working space\n",
    "    os.system(f\"gsutil cp '{my_bucket}/data/{folder}/{table_name}' .\")\n",
    "\n",
    "    print(f'[INFO] {table_name} is successfully downloaded into your working space')\n",
    "    # save dataframe in a csv file in the same workspace as the notebook\n",
    "    table_read = pd.read_csv(table_name, sep=\"\\t\")\n",
    "    return table_read\n",
    "def getFile(table_name, folder):\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file from the bucket to the current working space\n",
    "    os.system(f\"gsutil cp '{my_bucket}/data/{folder}/{table_name}' .\")\n",
    "\n",
    "    print(f'[INFO] {table_name} is successfully downloaded into your working space')\n",
    "def describe_tables(**tables):\n",
    "    # Initialize a dictionary to store results\n",
    "    descriptions = {}\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    # Loop through each table with its name\n",
    "    for table_name, table in tables.items():\n",
    "        \n",
    "        table_unique = table.drop_duplicates(\"person_id\", keep = \"first\")\n",
    "        \n",
    "        desc = table_unique[\"AgeAtVisit_Years\"].describe()\n",
    "\n",
    "        # Calculate male and female counts and mf_ratio\n",
    "        male_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Male\", na=False)].shape[0]\n",
    "        female_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Female\", na=False)].shape[0]\n",
    "        mf_ratio = male_count / (male_count + female_count) if (male_count + female_count) > 0 else None\n",
    "\n",
    "        # Add the calculated values to the description as new rows\n",
    "        desc[\"male_count\"] = male_count\n",
    "        desc[\"female_count\"] = female_count\n",
    "        desc[\"mf_ratio\"] = mf_ratio\n",
    "        \n",
    "        #calculate solution annotated rate\n",
    "        \n",
    "\n",
    "        # Store the updated description in the dictionary\n",
    "        descriptions[table_name] = desc\n",
    "        i+=1\n",
    "\n",
    "    # Convert the dictionary of descriptions to a DataFrame\n",
    "    result_df = pd.DataFrame(descriptions).T  # Transpose to have table names as rows\n",
    "    result_df.index.name = \"Table_Name\"  # Set a meaningful index name\n",
    "    result_df = result_df.reset_index()\n",
    "\n",
    "    return result_df\n",
    "def annoConsentDate(table):\n",
    "    #annotate with consent dates\n",
    "\n",
    "    # Get the list of person IDs from the first query.\n",
    "    person_ids = table['person_id'].unique().tolist()\n",
    "    person_ids_query = ','.join(map(str, person_ids))\n",
    "\n",
    "    # Run the consent query using the person IDs from the first query.\n",
    "    consent_query = f'''\n",
    "    SELECT DISTINCT person_id, MAX(observation_date) AS primary_consent_date\n",
    "    FROM `{CDR}.concept`\n",
    "    JOIN `{CDR}.concept_ancestor` ON concept_id = ancestor_concept_id\n",
    "    JOIN `{CDR}.observation` ON descendant_concept_id = observation_source_concept_id\n",
    "    WHERE concept_name = 'Consent PII' \n",
    "      AND concept_class_id = 'Module'\n",
    "      AND person_id IN ({person_ids_query})\n",
    "    GROUP BY person_id\n",
    "    '''\n",
    "    consent_dates = pd.read_gbq(consent_query, progress_bar_type=\"tqdm_notebook\")\n",
    "\n",
    "    # Merge the results on person_id to incorporate the consent date.\n",
    "    final_result = table.merge(consent_dates, on='person_id', how='left')\n",
    "    \n",
    "    return final_result\n",
    "def describe_solution_tables(**tables):\n",
    "    # Initialize a dictionary to store results\n",
    "    descriptions = {}\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    # Loop through each table with its name\n",
    "    for table_name, table in tables.items():\n",
    "        \n",
    "        table_unique = table.drop_duplicates(\"person_id\", keep = \"first\")\n",
    "        \n",
    "        #table_unique = calcAgeAtConsent(table_unique)\n",
    "        \n",
    "        desc = table_unique[\"AgeAtConsent_Years\"].describe()\n",
    "\n",
    "        # Calculate male and female counts and mf_ratio\n",
    "        male_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Male\", na=False)].shape[0]\n",
    "        female_count = table_unique[table_unique[\"sex_at_birth_source_value\"].str.contains(\"SexAtBirth_Female\", na=False)].shape[0]\n",
    "        mf_ratio = male_count / (male_count + female_count) if (male_count + female_count) > 0 else None\n",
    "\n",
    "        # Add the calculated values to the description as new rows\n",
    "        desc[\"male_count\"] = male_count\n",
    "        desc[\"female_count\"] = female_count\n",
    "        desc[\"mf_ratio\"] = mf_ratio\n",
    "        \n",
    "        #calculate solution annotated rate\n",
    "        \n",
    "\n",
    "        # Store the updated description in the dictionary\n",
    "        descriptions[table_name] = desc\n",
    "        i+=1\n",
    "\n",
    "    # Convert the dictionary of descriptions to a DataFrame\n",
    "    result_df = pd.DataFrame(descriptions).T  # Transpose to have table names as rows\n",
    "    result_df.index.name = \"Table_Name\"  # Set a meaningful index name\n",
    "    result_df = result_df.reset_index()\n",
    "\n",
    "    return result_df\n",
    "def calcAgeToday(solution_person_table):\n",
    "    # Create a copy of the input DataFrame to avoid modifying the original\n",
    "    solution_person_table = solution_person_table.copy()\n",
    "\n",
    "    # Ensure 'birth_datetime' is in datetime format and make it timezone-naive\n",
    "    solution_person_table[\"birth_datetime\"] = pd.to_datetime(\n",
    "        solution_person_table[\"birth_datetime\"], errors=\"coerce\"\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    # Check for invalid or missing dates\n",
    "    if solution_person_table[\"birth_datetime\"].isnull().any():\n",
    "        print(\"Warning: Missing or invalid birth_datetime values detected.\")\n",
    "        print(solution_person_table[solution_person_table[\"birth_datetime\"].isnull()])\n",
    "\n",
    "    # Drop rows with invalid or missing 'birth_datetime'\n",
    "    solution_person_table = solution_person_table.dropna(subset=[\"birth_datetime\"])\n",
    "\n",
    "    # Get today's date as timezone-naive\n",
    "    today = datetime.now()\n",
    "\n",
    "    # Calculate the difference (result is a timedelta Series)\n",
    "    solution_person_table[\"AgeToday\"] = today - solution_person_table[\"birth_datetime\"]\n",
    "\n",
    "    # Ensure 'AgeToday' is a timedelta type\n",
    "    if not pd.api.types.is_timedelta64_dtype(solution_person_table[\"AgeToday\"]):\n",
    "        raise ValueError(\"AgeToday is not a valid timedelta64 dtype.\")\n",
    "\n",
    "    # Convert the timedelta to years\n",
    "    solution_person_table[\"AgeToday_Years\"] = (\n",
    "        solution_person_table[\"AgeToday\"].dt.days / 365.25\n",
    "    )\n",
    "\n",
    "    return solution_person_table\n",
    "def calcAgeAtConsent(ICD_person_table):\n",
    "    # Calculate the difference (result is a timedelta Series)\n",
    "    ICD_person_table[\"AgeAtConsent\"] = (\n",
    "        ICD_person_table[\"primary_consent_date\"] - ICD_person_table[\"birth_datetime\"]\n",
    "    )\n",
    "\n",
    "    # Convert the timedelta to years\n",
    "    ICD_person_table[\"AgeAtConsent_Years\"] = (\n",
    "        ICD_person_table[\"AgeAtConsent\"].dt.days / 365.25\n",
    "    )\n",
    "    \n",
    "    return ICD_person_table\n",
    "def calc_solved_rate(summary_df2):\n",
    "\n",
    "    concept_solved_n = summary_df2[~summary_df2['Table_Name'].str.contains(\"concept|female\", case=False)][\"count\"].sum()\n",
    "    summary_df2 = summary_df2.reset_index()\n",
    "    #print(summary_df2)\n",
    "    #print(concept_solved_n)\n",
    "\n",
    "    concept_n = summary_df2.loc[0,\"count\"]\n",
    "    #print(concept_n)\n",
    "    concept_solved_rate = concept_solved_n / concept_n\n",
    "\n",
    "    summary_df2.loc[0,\"solved_rate\"] = concept_solved_rate\n",
    "\n",
    "    subset_names = [\"AD\", \"XLmale\", \"XLfemale\", \"AR\", \"ADAR\"]\n",
    "    k=1\n",
    "    for i in subset_names:\n",
    "        i_reg = f\"^{i}_\"\n",
    "        i_tablename = f\"{i}_solutions\"\n",
    "\n",
    "        #print(f\"Table Name: {i_tablename}\")\n",
    "\n",
    "        # Access and print the DataFrame dynamically\n",
    "        if i_tablename in globals():\n",
    "            tbl = globals()[i_tablename]\n",
    "            tot_sol = tbl[\"person_id\"].nunique()\n",
    "            sol_anno = summary_df2[summary_df2[\"Table_Name\"].str.contains(i_reg, case=False)][\"count\"].sum()\n",
    "            sol_anno_rate = sol_anno / tot_sol\n",
    "            summary_df2.loc[k,\"solved_rate\"] = sol_anno_rate\n",
    "        else:\n",
    "            print(f\"{i_tablename} does not exist.\")\n",
    "        k+=1\n",
    "\n",
    "    return summary_df2 \n",
    "def calc_solved_rate_within(summary_df2):\n",
    "\n",
    "    concept_solved_n = summary_df2[~summary_df2['Table_Name'].str.contains(\"concept|female\", case=False)][\"count\"].sum()\n",
    "    summary_df2 = summary_df2.reset_index()\n",
    "    #print(summary_df2)\n",
    "    #print(concept_solved_n)\n",
    "\n",
    "    concept_n = summary_df2.loc[0,\"count\"]\n",
    "    #print(concept_n)\n",
    "    concept_solved_rate = concept_solved_n / concept_n\n",
    "\n",
    "    summary_df2.loc[0,\"solved_rate\"] = concept_solved_rate\n",
    "\n",
    "    subset_names = [\"AD\", \"XLmale\", \"XLfemale\", \"AR\", \"ADAR\"]\n",
    "    k=1\n",
    "    for i in subset_names:\n",
    "        i_reg = f\"^{i}_\"\n",
    "        i_tablename = f\"{i}_solutions\"\n",
    "\n",
    "        #print(f\"Table Name: {i_tablename}\")\n",
    "\n",
    "        # Access and print the DataFrame dynamically\n",
    "        if i_tablename in globals():\n",
    "            tbl = globals()[i_tablename]\n",
    "            tot_sol = tbl[\"person_id\"].nunique()\n",
    "            sol_anno = summary_df2[summary_df2[\"Table_Name\"].str.contains(i_reg, case=False)][\"count\"].sum()\n",
    "            sol_anno_rate = sol_anno / concept_n\n",
    "            summary_df2.loc[k,\"solved_rate\"] = sol_anno_rate\n",
    "        else:\n",
    "            print(f\"{i_tablename} does not exist.\")\n",
    "        k+=1\n",
    "\n",
    "    return summary_df2 \n",
    "def generate_merges(base_name, concept_person_table, solution_dict):\n",
    "    \"\"\"\n",
    "    Dynamically generate merged tables, add them to the global namespace, and return a list of table names.\n",
    "\n",
    "    Parameters:\n",
    "    - base_name (str): The base name to replace \"retdegen\" in the generated table names.\n",
    "    - concept_person_table (pd.DataFrame): The concept-person DataFrame to merge on.\n",
    "    - solution_dict (dict): A dictionary where keys are solution table names (e.g., \"AD_solutions\")\n",
    "                            and values are the corresponding DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of dynamically generated table names.\n",
    "    \"\"\"\n",
    "    # Initialize the list to store table names\n",
    "    table_names = []\n",
    "    \n",
    "    # Remove conflicting global variables\n",
    "    #for key in list(globals().keys()):\n",
    "    #    if key.endswith(\"_in_retdegen\") or key == \"retdegen_concept_person\":\n",
    "    #        print(f\"Removing stale variable: {key}\")\n",
    "    #        del globals()[key]\n",
    "\n",
    "    for solution_name, solution_table in solution_dict.items():\n",
    "        # Construct the target table name dynamically\n",
    "        target_table_name = f\"{solution_name.split('_')[0]}_in_{base_name}\"\n",
    "        table_names.append(target_table_name)  # Add the table name to the list\n",
    "        \n",
    "\n",
    "        # Perform the merge\n",
    "        merged_table = pd.merge(\n",
    "            solution_table[['variant_id', 'person_id', 'allele_count', 'GeneID_EG', 'GeneID_Symbol']],\n",
    "            concept_person_table,\n",
    "            on=\"person_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Assign the result to the global namespace\n",
    "        globals()[target_table_name] = merged_table\n",
    "        print(f\"Generated table: {target_table_name}\")\n",
    "\n",
    "    # Add the concept-person table to the list of table names\n",
    "    concept_table_name = f\"{base_name}_concept_person\"\n",
    "    table_names.insert(0, concept_table_name)  # Ensure it appears first in the list\n",
    "    globals()[concept_table_name] = concept_person_table  # Assign it to the global namespace\n",
    "    #print(f\"Generated table: {concept_table_name}\")\n",
    "\n",
    "    return table_names\n",
    "def process_concept_and_solution_tables(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concept and solution tables.\n",
    "    \"\"\"\n",
    "    all_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Describe the merged tables\n",
    "        summary_df = describe_tables(**merged_tables)\n",
    "\n",
    "        # Calculate solved rates\n",
    "        solved_summary = calc_solved_rate(summary_df)\n",
    "\n",
    "        # Add a column to identify the concept\n",
    "        solved_summary[\"concept\"] = concept_name\n",
    "\n",
    "        # Append the summary to the list\n",
    "        all_summaries.append(solved_summary)\n",
    "\n",
    "    # Combine all summaries into one big table\n",
    "    final_summary_table = pd.concat(all_summaries, axis=0)\n",
    "\n",
    "    return final_summary_table\n",
    "def process_concept_and_solution_tables_within(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concept and solution tables.\n",
    "    \"\"\"\n",
    "    all_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Describe the merged tables\n",
    "        summary_df = describe_tables(**merged_tables)\n",
    "\n",
    "        # Calculate solved rates\n",
    "        solved_summary = calc_solved_rate_within(summary_df)\n",
    "\n",
    "        # Add a column to identify the concept\n",
    "        solved_summary[\"concept\"] = concept_name\n",
    "\n",
    "        # Append the summary to the list\n",
    "        all_summaries.append(solved_summary)\n",
    "\n",
    "    # Combine all summaries into one big table\n",
    "    final_summary_table = pd.concat(all_summaries, axis=0)\n",
    "\n",
    "    return final_summary_table\n",
    "def process_concept_solution_gene_summaries(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates gene-level summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concepts, solutions, and genes.\n",
    "    \"\"\"\n",
    "    all_gene_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Loop through each merged table\n",
    "        for table_name, merged_table in merged_tables.items():\n",
    "            # Ensure 'GeneID_Symbol' exists in the merged table\n",
    "            if \"GeneID_Symbol\" not in merged_table.columns:\n",
    "                print(f\"Warning: 'GeneID_Symbol' column not found in {table_name}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Group by GeneID_Symbol and describe each group\n",
    "            gene_groups = merged_table.groupby(\"GeneID_Symbol\")\n",
    "\n",
    "\n",
    "            for gene, group in gene_groups:\n",
    "                # Generate summaries for the current gene\n",
    "                \n",
    "                # find all NaN entries in condition i.e. unannotated\n",
    "                nan_mask = group.condition_source_value.isnull() # or df.isna()\n",
    "                \n",
    "                summary = {\n",
    "                    \"concept\": concept_name,\n",
    "                    \"table\": table_name,\n",
    "                    \"gene\": gene,\n",
    "                    \"variant_count\":  group[~nan_mask].variant_id.nunique(),\n",
    "                    \"person_count\": group[~nan_mask].person_id.nunique()\n",
    "                    #\"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                    #\"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                    #\"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                    #\"allele_count_max\": group[\"allele_count\"].max(),\n",
    "                }\n",
    "                # Append the summary to the list\n",
    "                all_gene_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    gene_summary_table = pd.DataFrame(all_gene_summaries)\n",
    "    \n",
    "    gene_summary_table['solution_table'] = gene_summary_table['table'].str.split('_').str[0]\n",
    "\n",
    "\n",
    "    return gene_summary_table\n",
    "def process_solution_gene_summaries(solution_tables):\n",
    "    \"\"\"\n",
    "    Generates gene-level summaries directly from solution tables.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing gene-level summaries for all solution tables.\n",
    "    \"\"\"\n",
    "    all_gene_summaries = []\n",
    "\n",
    "    # Loop through each solution table\n",
    "    for table_name, solution_table in solution_tables.items():\n",
    "        # Ensure 'GeneID_Symbol' exists in the solution table\n",
    "        if \"GeneID_Symbol\" not in solution_table.columns:\n",
    "            print(f\"Warning: 'GeneID_Symbol' column not found in {table_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Group by GeneID_Symbol and describe each group\n",
    "        gene_groups = solution_table.groupby(\"GeneID_Symbol\")\n",
    "\n",
    "        for gene, group in gene_groups:\n",
    "            # Generate summaries for the current gene\n",
    "            summary = {\n",
    "                \"table\": table_name,\n",
    "                \"gene\": gene,\n",
    "                \"variant_count\": group[\"variant_id\"].nunique(),\n",
    "                \"person_count\": group[\"person_id\"].nunique(),\n",
    "                \"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                \"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                \"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                \"allele_count_max\": group[\"allele_count\"].max(),\n",
    "            }\n",
    "            # Append the summary to the list\n",
    "            all_gene_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    gene_summary_table = pd.DataFrame(all_gene_summaries)\n",
    "\n",
    "    return gene_summary_table\n",
    "def calculate_annotation_rates(solution_gene_summary, concept_gene_summary):\n",
    "    \"\"\"\n",
    "    Calculates annotation rates at the gene and variant levels by comparing solution summaries\n",
    "    and concept-intersection summaries.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_gene_summary (pd.DataFrame): Gene-level summaries for solution tables \n",
    "                                             (output from process_solution_gene_summaries).\n",
    "    - concept_gene_summary (pd.DataFrame): Gene-level summaries for intersection tables \n",
    "                                           (output from process_concept_solution_gene_summaries).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing annotation rates at the gene and variant levels.\n",
    "    \"\"\"\n",
    "    # Merge the solution and concept summaries on the common columns\n",
    "    merged = pd.merge(\n",
    "        concept_gene_summary,\n",
    "        solution_gene_summary,\n",
    "        on=[\"table\", \"gene\"],\n",
    "        suffixes=(\"_concept\", \"_solution\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Calculate annotation rates\n",
    "    merged[\"person_annotation_rate\"] = (\n",
    "        merged[\"person_count_concept\"] / merged[\"person_count_solution\"]\n",
    "    )\n",
    "    merged[\"variant_annotation_rate\"] = (\n",
    "        merged[\"variant_count_concept\"] / merged[\"variant_count_solution\"]\n",
    "    )\n",
    "\n",
    "    # Handle divisions by zero or missing values\n",
    "    merged[\"person_annotation_rate\"] = merged[\"person_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "    merged[\"variant_annotation_rate\"] = merged[\"variant_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "\n",
    "    # Return the annotated DataFrame\n",
    "    return merged\n",
    "def calculate_all_annotation_rates(solution_gene_summary, concept_gene_summary):\n",
    "    \"\"\"\n",
    "    Calculates annotation rates at the gene and variant levels for each category\n",
    "    by comparing solution and concept summaries.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_gene_summary (pd.DataFrame): Gene-level summaries for solution tables.\n",
    "    - concept_gene_summary (pd.DataFrame): Gene-level summaries for intersection tables.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing annotation rates at the gene and variant levels for each category.\n",
    "    \"\"\"\n",
    "    all_annotation_rates = []\n",
    "    \n",
    "    # Make necessary columns\n",
    "    concept_gene_summary[\"category\"] = concept_gene_summary[\"table\"].str.extract(r\"in_([a-zA-Z]+)\").fillna(\"\")\n",
    "    concept_gene_summary[\"category_inheritance\"] = concept_gene_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_in\").fillna(\"\")\n",
    "    solution_gene_summary[\"category\"] = solution_gene_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_solutions\").fillna(\"\")\n",
    "\n",
    "\n",
    "    # Ensure both summaries have the necessary columns\n",
    "    required_columns = {\"table\", \"gene\", \"person_count\", \"variant_count\", \"category\"}\n",
    "    if not required_columns.issubset(solution_gene_summary.columns):\n",
    "        raise ValueError(\"Solution gene summary is missing required columns.\")\n",
    "    if not required_columns.issubset(concept_gene_summary.columns):\n",
    "        raise ValueError(\"Concept gene summary is missing required columns.\")\n",
    "\n",
    "    # Process each category separately\n",
    "    for con_category in concept_gene_summary[\"category\"].unique():\n",
    "        # Filter concept summaries for the current category\n",
    "        concept_subset = concept_gene_summary[concept_gene_summary[\"category\"] == con_category]\n",
    "        \n",
    "        for sol_category in solution_gene_summary[\"category\"].unique():\n",
    "            # Filter solution summaries for the current category\n",
    "            sol_category_q = f\"^{sol_category}\"\n",
    "            consol_category_q = f\"^{sol_category}$\"\n",
    "            solution_subset = solution_gene_summary[solution_gene_summary[\"category\"].str.contains(sol_category_q)]\n",
    "            concept_subset_inh = concept_subset[concept_subset[\"category_inheritance\"].str.contains(consol_category_q)]\n",
    "\n",
    "            if concept_subset_inh.empty or solution_subset.empty:\n",
    "                print(f\"Skipping category '{con_category}' / '{sol_category}' because one of the subsets is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Debugging: Check subset details\n",
    "            print(f\"Processing category '{con_category}' / '{sol_category}'\")\n",
    "            #print(\"Concept Subset:\")\n",
    "            #print(concept_subset_inh.head())\n",
    "\n",
    "            # Merge the summaries for this category\n",
    "            merged = pd.merge(\n",
    "                concept_subset_inh,\n",
    "                solution_subset[['variant_count', 'person_count', 'gene']],\n",
    "                on=[\"gene\"],\n",
    "                suffixes=(\"_concept\", \"_solution\"),\n",
    "                how=\"inner\"\n",
    "            )\n",
    "\n",
    "            # Calculate annotation rates\n",
    "            merged[\"person_annotation_rate\"] = (\n",
    "                merged[\"person_count_concept\"] / merged[\"person_count_solution\"]\n",
    "            )\n",
    "            merged[\"variant_annotation_rate\"] = (\n",
    "                merged[\"variant_count_concept\"] / merged[\"variant_count_solution\"]\n",
    "            )\n",
    "\n",
    "            # Handle divisions by zero or missing values\n",
    "            merged[\"person_annotation_rate\"] = merged[\"person_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "            merged[\"variant_annotation_rate\"] = merged[\"variant_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "\n",
    "            # Append to the results list\n",
    "            all_annotation_rates.append(merged)\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    if all_annotation_rates:\n",
    "        final_annotation_rates = pd.concat(all_annotation_rates, axis=0, ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        final_annotation_rates = pd.DataFrame()  # Return an empty DataFrame if no data\n",
    "    \n",
    "    return final_annotation_rates\n",
    "def process_solution_variant_summaries(solution_tables):\n",
    "    \"\"\"\n",
    "    Generates variant-level summaries directly from solution tables.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing variant-level summaries for all solution tables.\n",
    "    \"\"\"\n",
    "    all_variant_summaries = []\n",
    "\n",
    "    # Loop through each solution table\n",
    "    for table_name, solution_table in solution_tables.items():\n",
    "        # Ensure 'variant_id' exists in the solution table\n",
    "        if \"variant_id\" not in solution_table.columns:\n",
    "            print(f\"Warning: 'variant_id' column not found in {table_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Group by variant_id and describe each group\n",
    "        variant_groups = solution_table.groupby(\"variant_id\")\n",
    "\n",
    "        for variant, group in variant_groups:\n",
    "            # Generate summaries for the current variant\n",
    "            summary = {\n",
    "                \"table\": table_name,\n",
    "                \"variant_id\": variant,\n",
    "                \"gene\": group[\"GeneID_Symbol\"].iloc[0] if \"GeneID_Symbol\" in group.columns else None,\n",
    "                \"person_count\": group[\"person_id\"].nunique(),\n",
    "                \"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                \"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                \"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                \"allele_count_max\": group[\"allele_count\"].max(),\n",
    "            }\n",
    "            # Append the summary to the list\n",
    "            all_variant_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    variant_summary_table = pd.DataFrame(all_variant_summaries)\n",
    "\n",
    "    return variant_summary_table\n",
    "def process_concept_solution_variant_summaries(concept_tables, solution_tables):\n",
    "    \"\"\"\n",
    "    Loops through each concept_table and solution_table, generates merged tables,\n",
    "    calculates variant-level summaries, and combines all summaries into one big table.\n",
    "\n",
    "    Parameters:\n",
    "    - concept_tables (dict): Dictionary of concept tables (key: table name, value: DataFrame).\n",
    "    - solution_tables (dict): Dictionary of solution tables (key: table name, value: DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A single merged summary table containing results for all concepts, solutions, and variants.\n",
    "    \"\"\"\n",
    "    all_variant_summaries = []\n",
    "\n",
    "    # Loop through each concept table\n",
    "    for concept_name, concept_table in concept_tables.items():\n",
    "        base_name = concept_name.split(\"_\")[0]  # Extract base name (e.g., \"retdys\")\n",
    "\n",
    "        # Generate merged tables for this concept table\n",
    "        merged_table_names = generate_merges(base_name, concept_table, solution_tables)\n",
    "\n",
    "        # Retrieve the generated tables from the global namespace\n",
    "        merged_tables = {name: globals()[name] for name in merged_table_names}\n",
    "\n",
    "        # Loop through each merged table\n",
    "        for table_name, merged_table in merged_tables.items():\n",
    "            # Ensure 'variant_id' exists in the merged table\n",
    "            if \"variant_id\" not in merged_table.columns:\n",
    "                print(f\"Warning: 'variant_id' column not found in {table_name}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Group by variant_id and describe each group\n",
    "            variant_groups = merged_table.groupby(\"variant_id\")\n",
    "\n",
    "            for variant, group in variant_groups:\n",
    "                # Generate summaries for the current variant\n",
    "                \n",
    "                # find all NaN entries in condition i.e. unannotated\n",
    "                nan_mask = group.condition_source_value.isnull() # or df.isna()\n",
    "                \n",
    "                summary = {\n",
    "                    \"concept\": concept_name,\n",
    "                    \"table\": table_name,\n",
    "                    \"variant_id\": variant,\n",
    "                    \"gene\": group[\"GeneID_Symbol\"].iloc[0] if \"GeneID_Symbol\" in group.columns else None,\n",
    "                    \"person_count\": group[~nan_mask].person_id.nunique()\n",
    "                    #\"allele_count_mean\": group[\"allele_count\"].mean(),\n",
    "                    #\"allele_count_std\": group[\"allele_count\"].std(),\n",
    "                    #\"allele_count_min\": group[\"allele_count\"].min(),\n",
    "                    #\"allele_count_max\": group[\"allele_count\"].max(),\n",
    "                }\n",
    "                # Append the summary to the list\n",
    "                all_variant_summaries.append(summary)\n",
    "\n",
    "    # Convert the list of summaries into a DataFrame\n",
    "    variant_summary_table = pd.DataFrame(all_variant_summaries)\n",
    "\n",
    "    return variant_summary_table\n",
    "def calculate_all_variant_annotation_rates(solution_variant_summary, concept_variant_summary):\n",
    "    \"\"\"\n",
    "    Calculates annotation rates at the variant level for each category\n",
    "    by comparing solution and concept summaries.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_variant_summary (pd.DataFrame): Variant-level summaries for solution tables.\n",
    "    - concept_variant_summary (pd.DataFrame): Variant-level summaries for intersection tables.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing annotation rates at the variant level for each category.\n",
    "    \"\"\"\n",
    "    all_annotation_rates = []\n",
    "\n",
    "    # Add necessary columns for categorization\n",
    "    concept_variant_summary[\"category\"] = concept_variant_summary[\"table\"].str.extract(r\"in_([a-zA-Z]+)\").fillna(\"\")\n",
    "    concept_variant_summary[\"category_inheritance\"] = concept_variant_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_in\").fillna(\"\")\n",
    "    solution_variant_summary[\"category\"] = solution_variant_summary[\"table\"].str.extract(r\"([a-zA-Z]+)_solutions\").fillna(\"\")\n",
    "\n",
    "    # Ensure both summaries have the required columns\n",
    "    required_columns = {\"table\", \"variant_id\", \"person_count\", \"allele_count_mean\", \"category\"}\n",
    "    if not required_columns.issubset(solution_variant_summary.columns):\n",
    "        raise ValueError(\"Solution variant summary is missing required columns.\")\n",
    "    if not required_columns.issubset(concept_variant_summary.columns):\n",
    "        raise ValueError(\"Concept variant summary is missing required columns.\")\n",
    "\n",
    "    # Process each category separately\n",
    "    for con_category in concept_variant_summary[\"category\"].unique():\n",
    "        # Filter concept summaries for the current category\n",
    "        concept_subset = concept_variant_summary[concept_variant_summary[\"category\"] == con_category]\n",
    "\n",
    "        for sol_category in solution_variant_summary[\"category\"].unique():\n",
    "            # Filter solution summaries for the current category\n",
    "            sol_category_q = f\"^{sol_category}\"\n",
    "            consol_category_q = f\"^{sol_category}$\"\n",
    "            solution_subset = solution_variant_summary[solution_variant_summary[\"category\"].str.contains(sol_category_q)]\n",
    "            concept_subset_inh = concept_subset[concept_subset[\"category_inheritance\"].str.contains(consol_category_q)]\n",
    "\n",
    "            if concept_subset_inh.empty or solution_subset.empty:\n",
    "                print(f\"Skipping category '{con_category}' / '{sol_category}' because one of the subsets is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Debugging: Check subset details\n",
    "            print(f\"Processing category '{con_category}' / '{sol_category}'\")\n",
    "\n",
    "            # Merge the summaries for this category\n",
    "            merged = pd.merge(\n",
    "                concept_subset_inh,\n",
    "                solution_subset[['variant_id', 'person_count', 'allele_count_mean']],\n",
    "                on=[\"variant_id\"],\n",
    "                suffixes=(\"_concept\", \"_solution\"),\n",
    "                how=\"inner\"\n",
    "            )\n",
    "\n",
    "            # Calculate annotation rates\n",
    "            merged[\"person_annotation_rate\"] = (\n",
    "                merged[\"person_count_concept\"] / merged[\"person_count_solution\"]\n",
    "            )\n",
    "            merged[\"allele_annotation_rate\"] = (\n",
    "                merged[\"allele_count_mean_concept\"] / merged[\"allele_count_mean_solution\"]\n",
    "            )\n",
    "\n",
    "            # Handle divisions by zero or missing values\n",
    "            merged[\"person_annotation_rate\"] = merged[\"person_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "            merged[\"allele_annotation_rate\"] = merged[\"allele_annotation_rate\"].fillna(0).replace([float('inf')], 0)\n",
    "\n",
    "            # Append to the results list\n",
    "            all_annotation_rates.append(merged)\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    if all_annotation_rates:\n",
    "        final_annotation_rates = pd.concat(all_annotation_rates, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        final_annotation_rates = pd.DataFrame()  # Return an empty DataFrame if no data\n",
    "\n",
    "    return final_annotation_rates\n",
    "def saveToBucket(df, df_filename, data_folder):\n",
    "    df.to_csv(df_filename, sep = \"\\t\", index=False)\n",
    "\n",
    "    # get the bucket name\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file to the bucket\n",
    "    args = [\"gsutil\", \"cp\", f\"./{df_filename}\", f\"{my_bucket}/data/{data_folder}/\"]\n",
    "    output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "    # print output from gsutil\n",
    "    output.stderr\n",
    "def concatenate_variant_columns(df, chrom_col='CHROM', pos_col='POS', ref_col='REF', alt_col='ALT', new_col='Variant'):\n",
    "    \"\"\"\n",
    "    Concatenates CHROM, POS, REF, and ALT columns into a single Variant column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the columns to concatenate.\n",
    "    - chrom_col (str): Name of the chromosome column. Default is 'CHROM'.\n",
    "    - pos_col (str): Name of the position column. Default is 'POS'.\n",
    "    - ref_col (str): Name of the reference allele column. Default is 'REF'.\n",
    "    - alt_col (str): Name of the alternate allele column. Default is 'ALT'.\n",
    "    - new_col (str): Name of the new concatenated column. Default is 'Variant'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new concatenated column.\n",
    "    \"\"\"\n",
    "    # Ensure the required columns exist\n",
    "    required_cols = [chrom_col, pos_col, ref_col, alt_col]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n",
    "\n",
    "    # Convert all columns to string to ensure proper concatenation\n",
    "    df[new_col] = df[chrom_col].astype(str) + '-' + \\\n",
    "                 df[pos_col].astype(str) + '-' + \\\n",
    "                 df[ref_col].astype(str) + '-' + \\\n",
    "                 df[alt_col].astype(str)\n",
    "\n",
    "    return df\n",
    "def split_variant_column(df, variant_col='Variant', chrom_col='CHROM', pos_col='POS', ref_col='REF', alt_col='ALT', sep='-'):\n",
    "    \"\"\"\n",
    "    Splits a Variant column into CHROM, POS, REF, and ALT columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the Variant column.\n",
    "    - variant_col (str): Name of the concatenated Variant column. Default is 'Variant'.\n",
    "    - chrom_col (str): Name of the chromosome column to create. Default is 'CHROM'.\n",
    "    - pos_col (str): Name of the position column to create. Default is 'POS'.\n",
    "    - ref_col (str): Name of the reference allele column to create. Default is 'REF'.\n",
    "    - alt_col (str): Name of the alternate allele column to create. Default is 'ALT'.\n",
    "    - sep (str): Separator used in the Variant column. Default is '-'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the split columns.\n",
    "    \"\"\"\n",
    "    if variant_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{variant_col}' not found in the DataFrame.\")\n",
    "\n",
    "    # Split the Variant column into multiple columns\n",
    "    split_cols = df[variant_col].str.split(sep, expand=True)\n",
    "\n",
    "    if split_cols.shape[1] != 4:\n",
    "        raise ValueError(f\"Expected 4 components in the Variant column separated by '{sep}', but got {split_cols.shape[1]}.\")\n",
    "\n",
    "    # Assign the split columns to respective new columns\n",
    "    df[chrom_col] = split_cols[0]\n",
    "    df[pos_col] = split_cols[1]\n",
    "    df[ref_col] = split_cols[2]\n",
    "    df[alt_col] = split_cols[3]\n",
    "\n",
    "    return df\n",
    "def annotate_variants(\n",
    "    anno_vars_table: pd.DataFrame,\n",
    "    clinvar_query_table: pd.DataFrame,\n",
    "    VAT_query_table: pd.DataFrame,\n",
    "    remove_list: list = None,\n",
    "    filter_canonical: bool = True,\n",
    "    drop_original_variant_columns: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Annotate a table of genetic variants with ClinVar and VAT annotations.\n",
    "\n",
    "    Parameters:\n",
    "    - anno_vars_table (pd.DataFrame): DataFrame containing variants to annotate. Must include 'variant_id'.\n",
    "    - clinvar_query_table (pd.DataFrame): DataFrame containing ClinVar annotations.\n",
    "    - VAT_query_table (pd.DataFrame): DataFrame containing VAT annotations.\n",
    "    - remove_list (list, optional): List of transcript consequences to exclude. Defaults to None.\n",
    "    - filter_canonical (bool, optional): If True, filter VAT to canonical transcripts. Defaults to True.\n",
    "    - drop_original_variant_columns (bool, optional): If True, drop 'VARIANT_forQuery' and 'vid' after merging. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Annotated DataFrame with ClinVar and VAT information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input DataFrames\n",
    "    required_clinvar_cols = {'VARIANT_forQuery', 'CLNHGVS', 'CLNREVSTAT', \n",
    "                             'CLNSIG', 'MC', 'Type', 'Name', 'RCVaccession'}\n",
    "    if not required_clinvar_cols.issubset(clinvar_query_table.columns):\n",
    "        missing = required_clinvar_cols - set(clinvar_query_table.columns)\n",
    "        raise ValueError(f\"ClinVar query table is missing columns: {missing}\")\n",
    "    \n",
    "    required_VAT_cols = {'vid', 'gvs_all_af', 'transcript_source', 'transcript', \n",
    "                        'dna_change_in_transcript', 'aa_change', 'consequence',\n",
    "                        'variant_type', 'exon_number', 'intron_number', 'gene_id',\n",
    "                        'revel', 'splice_ai_acceptor_gain_score', \n",
    "                        'splice_ai_acceptor_loss_score',\n",
    "                        'splice_ai_donor_gain_score', \n",
    "                        'splice_ai_donor_loss_score',\n",
    "                        'omim_phenotypes_id', 'omim_phenotypes_name',\n",
    "                        'clinvar_classification', 'entrezgene', 'is_canonical_transcript'}\n",
    "    if not required_VAT_cols.issubset(VAT_query_table.columns):\n",
    "        missing = required_VAT_cols - set(VAT_query_table.columns)\n",
    "        raise ValueError(f\"VAT query table is missing columns: {missing}\")\n",
    "    \n",
    "    # Validate anno_vars_table\n",
    "    if 'variant_id' not in anno_vars_table.columns:\n",
    "        raise ValueError(\"anno_vars_table must contain a 'variant_id' column.\")\n",
    "    \n",
    "    # Step 1: Merge with ClinVar Query Table\n",
    "    # Select necessary columns from ClinVar\n",
    "    clinvar_cols_to_merge = ['VARIANT_forQuery', 'CLNHGVS', 'CLNREVSTAT', 'ID',\n",
    "                             'CLNSIG', 'MC', 'Type', 'Name', 'RCVaccession']\n",
    "    \n",
    "    # Perform the merge\n",
    "    merged_df = anno_vars_table.merge(\n",
    "        clinvar_query_table[clinvar_cols_to_merge],\n",
    "        how='left',\n",
    "        left_on='variant_id',\n",
    "        right_on='VARIANT_forQuery'\n",
    "    )\n",
    "    \n",
    "    # Drop 'VARIANT_forQuery' if required\n",
    "    if drop_original_variant_columns:\n",
    "        merged_df.drop(columns=['VARIANT_forQuery'], inplace=True)\n",
    "    \n",
    "    # Step 2: Filter VAT Query Table\n",
    "    VAT_query_table_filtered = VAT_query_table.copy()\n",
    "    \n",
    "    if filter_canonical:\n",
    "        # Filter to canonical transcripts\n",
    "        if 'is_canonical_transcript' not in VAT_query_table_filtered.columns:\n",
    "            raise ValueError(\"VAT_query_table must contain 'is_canonical_transcript' column for filtering.\")\n",
    "        VAT_query_table_filtered = VAT_query_table_filtered[VAT_query_table_filtered[\"is_canonical_transcript\"] == True]\n",
    "    elif remove_list is not None:\n",
    "        # Remove specified transcript consequences\n",
    "        VAT_query_table_filtered = VAT_query_table_filtered[~VAT_query_table_filtered[\"consequence\"].isin(remove_list)]\n",
    "    \n",
    "    # Step 3: Merge with VAT Query Table\n",
    "    # Select necessary columns from VAT\n",
    "    VAT_cols_to_merge = [\n",
    "        'vid', 'gvs_all_af', 'transcript_source', 'transcript', \n",
    "        'dna_change_in_transcript', 'aa_change', 'consequence',\n",
    "        'variant_type', 'exon_number', 'intron_number', 'gene_id',\n",
    "        'revel', 'splice_ai_acceptor_gain_score', \n",
    "        'splice_ai_acceptor_loss_score',\n",
    "        'splice_ai_donor_gain_score', \n",
    "        'splice_ai_donor_loss_score',\n",
    "        'omim_phenotypes_id', 'omim_phenotypes_name',\n",
    "        'clinvar_classification', 'entrezgene'\n",
    "    ]\n",
    "    \n",
    "    merged_df_vat = merged_df.merge(\n",
    "        VAT_query_table_filtered[VAT_cols_to_merge],\n",
    "        how='left',\n",
    "        left_on='variant_id',\n",
    "        right_on='vid'\n",
    "    )\n",
    "    \n",
    "    # Drop 'vid' if required\n",
    "    if drop_original_variant_columns:\n",
    "        merged_df_vat.drop(columns=['vid'], inplace=True)\n",
    "    \n",
    "    # Optional: Handle multiple VAT annotations per variant_id\n",
    "    # Depending on the data structure, you might want to aggregate or handle duplicates\n",
    "    \n",
    "    return merged_df_vat\n",
    "def processConsentDate(tbl):\n",
    "    #gets consent date for each participant in any table with person_id and calculates age at consent\n",
    "    tbl = annoConsentDate(tbl)\n",
    "    tbl['birth_datetime'] = pd.to_datetime(tbl['birth_datetime'])\n",
    "    tbl['primary_consent_date'] = pd.to_datetime(tbl['primary_consent_date'])\n",
    "    tbl['primary_consent_date'] = tbl['primary_consent_date'].dt.tz_localize('UTC')\n",
    "    tbl = calcAgeAtConsent(tbl)\n",
    "    return tbl\n",
    "def procRace(df):\n",
    "    df.loc[df['race_source_value'].isin([\"AoUDRC_NoneIndicated\",\n",
    "                                   \"PMI_Skip\",\n",
    "                                   \"PMI_PreferNotToAnswer\"]), 'race_source_value'] = \"Unknown\"\n",
    "\n",
    "\n",
    "    df.loc[df['race_source_value'].isin([\"WhatRaceEthnicity_GeneralizedMultPopulations\",\n",
    "                                   \"WhatRaceEthnicity_RaceEthnicityNoneOfThese\"]), 'race_source_value'] = \"Other\"\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def makecontable_bygene(gene, \n",
    "                        anno_rates_table,\n",
    "                        concept = \"screenretdys_concept_person\", \n",
    "                        pop_size = 317964\n",
    "                        ):\n",
    "    #make a 2x2 contingency table for each gene within each table\"\n",
    "    #concept = \"retdegen_concept_person\"\n",
    "    con_table = get_global_var(concept)\n",
    "    #sol_table_name = \"full_table\"\n",
    "    #sol_table = get_global_var(sol_table_name)\n",
    "    #anno_rates_table = get_global_var(anno_rates_table_name)\n",
    "\n",
    "\n",
    "    con_sol_table = anno_rates_table[anno_rates_table[\"concept\"]==concept]\n",
    "    #genes = full_table[\"gene\"].unique()\n",
    "    #gene = genes[2]\n",
    "   \n",
    "\n",
    "    Sp = con_sol_table[con_sol_table.gene == gene][\"person_count_solution\"].item()\n",
    "    Cp = con_table.shape[0]\n",
    "\n",
    "    SpCp = con_sol_table[con_sol_table.gene == gene][\"person_count_concept\"].item()\n",
    "    SpCn = Sp - SpCp\n",
    "\n",
    "    Cn = pop_size - Cp\n",
    "    Sn = pop_size - Sp\n",
    "    SnCp = Cp - SpCp\n",
    "    SnCn = Sn - SnCp\n",
    "\n",
    "    table = np.array([[SpCp, SpCn],\n",
    "                    [SnCp, SnCn]])\n",
    "    \n",
    "    return([table, pop_size, Cp, Sp, SpCp])\n",
    "\n",
    "def loop_through_fisher_bygene(anno_rates_table, concept = \"screenretdys_concept_person\", alpha = 0.05):\n",
    "    #produce fisher tests for specific concepts by gene\n",
    "    results = []\n",
    "\n",
    "    # make fisher test table \n",
    "    # concept_array = anno_rates_table[\"concept\"].unique()\n",
    "    anno_rates_table = anno_rates_table.sort_values(by = \"person_count_solution\", ascending = False)\n",
    "    gene_array = anno_rates_table[anno_rates_table.concept == concept].gene.unique()\n",
    "    \n",
    "\n",
    "    \n",
    "    # alpha_corrected = alpha / concept_array.shape[0] if you want corrected 95CI\n",
    "    # better to adjust p-values directly\n",
    "    \n",
    "    n_tests = anno_rates_table.shape[0] #for bonf correction for all of the tests run\n",
    "    n_tests = gene_array.shape[0] #for bonf correction for all of the tests run\n",
    "\n",
    "\n",
    "    for gene in gene_array:\n",
    "        table_stats = makecontable_bygene(gene = gene, \n",
    "                                          concept = concept, \n",
    "                                          anno_rates_table = anno_rates_table)\n",
    "        fisher_table = table_stats[0]\n",
    "        pop_size = table_stats[1]\n",
    "        concept_size = table_stats[2]\n",
    "        num_IRDgenotype = table_stats[3]\n",
    "        num_annotated = table_stats[4]\n",
    "        concept_prevalence = concept_size/pop_size\n",
    "        DAR = num_annotated / num_IRDgenotype\n",
    "\n",
    "        # Fisher's exact test (returns odds ratio and p-value)\n",
    "        odds_ratio, p_value = fisher_exact(fisher_table)\n",
    "        p_value = p_value*n_tests\n",
    "\n",
    "        # Use Table2x2 from statsmodels to calculate the 95% confidence interval for the odds ratio\n",
    "        table2x2 = Table2x2(fisher_table)\n",
    "        conf_int = table2x2.oddsratio_confint(alpha=alpha)\n",
    "        lower_ci, upper_ci = conf_int[0], conf_int[1]\n",
    "        \n",
    "        # calculate concept prevlaence and DAR\n",
    "        concept_prevalence = table_stats[2]/table_stats[1]\n",
    "\n",
    "        # Append results: you can also store the p_value or table if needed\n",
    "        results.append({\n",
    "            \"Test\": gene,\n",
    "            \"Concept\": concept,\n",
    "            \"Odds_Ratio\": odds_ratio,\n",
    "            \"Lower_CI\": lower_ci,\n",
    "            \"Upper_CI\": upper_ci,\n",
    "            \"p_value\": p_value,\n",
    "            \"Pop_size\" : pop_size,\n",
    "            \"num_IRDgenotype\" : num_IRDgenotype,\n",
    "            \"Concept_size\" : concept_size,\n",
    "            \"Num_annotated\" : num_annotated,\n",
    "            \"Concept_prevalence\" : concept_prevalence,\n",
    "            \"DAR\": DAR\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return(df_results)\n",
    "\n",
    "\n",
    "def get_global_var(var_name):\n",
    "    # Retrieve the variable from the global namespace using the variable name (string)\n",
    "    return globals().get(var_name)\n",
    "\n",
    "def makecontable(anno_rates_table, concept, pop_size = 317964, sol_table_name = \"ALL_IRD\"):\n",
    "    #make a 2x2 contingency table for given solution & concept\"\n",
    "    con_table = get_global_var(concept)\n",
    "    sol_table = get_global_var(sol_table_name)\n",
    "    con_sol_table = anno_rates_table[anno_rates_table[\"concept\"]==concept]\n",
    "\n",
    "    Sp = sol_table.shape[0]\n",
    "    Cp = con_table.shape[0]\n",
    "\n",
    "    SpCp = sum(con_sol_table.person_count_concept)\n",
    "    SpCn = Sp - SpCp\n",
    "\n",
    "    Cn = pop_size - Cp\n",
    "    Sn = pop_size - Sp\n",
    "    SnCp = Cp - SpCp\n",
    "    SnCn = Sn - SnCp\n",
    "\n",
    "    table = np.array([[SpCp, SpCn],\n",
    "                    [SnCp, SnCn]])\n",
    "    \n",
    "    return([table, pop_size, Cp, Sp, SpCp])\n",
    "\n",
    "##test = makecontable(concept = \"retdegen_concept_person\", anno_rates_table = annotation_rates_solution_gene)\n",
    "##print(test[3])\n",
    "def loop_through_fisher(anno_rates_table, alpha = 0.05):\n",
    "    results = []\n",
    "\n",
    "    # make fisher test table \n",
    "    concept_array = anno_rates_table[\"concept\"].unique()\n",
    "    # alpha_corrected = alpha / concept_array.shape[0] if you want corrected 95CI\n",
    "    # better to adjust p-values directly\n",
    "    \n",
    "    n_tests = concept_array.shape[0]\n",
    "\n",
    "    for concept in concept_array:\n",
    "        table_stats = makecontable(concept = concept, anno_rates_table = anno_rates_table)\n",
    "        fisher_table = table_stats[0]\n",
    "        pop_size = table_stats[1]\n",
    "        concept_size = table_stats[2]\n",
    "        num_IRDgenotype = table_stats[3]\n",
    "        num_annotated = table_stats[4]\n",
    "        concept_prevalence = concept_size/pop_size\n",
    "        DAR = num_annotated / num_IRDgenotype\n",
    "\n",
    "        # Fisher's exact test (returns odds ratio and p-value)\n",
    "        odds_ratio, p_value = fisher_exact(fisher_table)\n",
    "        p_value = p_value*n_tests\n",
    "\n",
    "        # Use Table2x2 from statsmodels to calculate the 95% confidence interval for the odds ratio\n",
    "        table2x2 = Table2x2(fisher_table)\n",
    "        conf_int = table2x2.oddsratio_confint(alpha=alpha)\n",
    "        lower_ci, upper_ci = conf_int[0], conf_int[1]\n",
    "        \n",
    "        # calculate concept prevlaence and DAR\n",
    "        concept_prevalence = table_stats[2]/table_stats[1]\n",
    "\n",
    "        # Append results: you can also store the p_value or table if needed\n",
    "        results.append({\n",
    "            \"Test\": concept,\n",
    "            \"Odds_Ratio\": odds_ratio,\n",
    "            \"Lower_CI\": lower_ci,\n",
    "            \"Upper_CI\": upper_ci,\n",
    "            \"p_value\": p_value,\n",
    "            \"Pop_size\" : pop_size,\n",
    "            \"num_IRDgenotype\" : num_IRDgenotype,\n",
    "            \"Concept_size\" : concept_size,\n",
    "            \"Num_annotated\" : num_annotated,\n",
    "            \"Concept_prevalence\" : concept_prevalence,\n",
    "            \"DAR\": DAR\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return(df_results)\n",
    "\n",
    "def plot_fishertable_with_arrows(\n",
    "    df_fisher,\n",
    "    plot_title=\"Forest Plot of Fisher Exact Test Results\",\n",
    "    plot_output_file=\"forest_plot.png\",\n",
    "    logbase=2,\n",
    "    custom_labels=False,\n",
    "    dpi=300,\n",
    "    arrow_length=0.1,\n",
    "    offset_OR = 1.4, \n",
    "    offset_Pval = 7.5,\n",
    "    fig_width=10,         \n",
    "    fig_height=None,\n",
    "    x_max=None,\n",
    "    x_min=None,\n",
    "    pad = 0.5,\n",
    "    cap=None\n",
    "):\n",
    "    # find finite bounds for axis limits\n",
    "    finite_lowers = df_fisher[\"Lower_CI\"].replace(0, np.nan).dropna()\n",
    "    finite_uppers = df_fisher[\"Upper_CI\"].replace(np.inf, np.nan).dropna()\n",
    "    min_lower = finite_lowers.min()\n",
    "    max_upper = finite_uppers.max()\n",
    "    print(min_lower)\n",
    "    print(max_upper)\n",
    "\n",
    "    # set a tiny positive floor for log-axis\n",
    "    # floor = min_lower / 100.0\n",
    "    if x_min:\n",
    "        floor = x_min\n",
    "    else:\n",
    "        floor = 0.5\n",
    "    # set a cap for infinities\n",
    "    if cap is None:\n",
    "        cap = max_upper * 10\n",
    "\n",
    "    # y positions\n",
    "    n = len(df_fisher)\n",
    "    if fig_height is None:\n",
    "        fig_height = n * 0.5\n",
    "    y_positions = np.arange(n)\n",
    "\n",
    "    # start figure\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=dpi)\n",
    "    ax = plt.gca()\n",
    "    plt.xscale('log')\n",
    "    plt.axvline(1, color='red', linestyle='--')\n",
    "\n",
    "    # custom ticks\n",
    "    start_exp = int(np.floor(np.log(floor) / np.log(logbase)))\n",
    "    end_exp   = int(np.ceil(np.log(cap)   / np.log(logbase)))\n",
    "    ticks = np.logspace(start_exp, end_exp, end_exp - start_exp + 1, base=logbase)\n",
    "    # Generate tick positions on a power-of-2 scale.\n",
    "    #ticks = np.logspace(start_exponent, end_exponent, num=end_exponent - start_exponent + 1, base=logbase)\n",
    "    # Create tick labels directly from ticks (convert to int if desired, or leave as float)\n",
    "    tick_labels = [str((t)) for t in ticks]\n",
    "    \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels([f\"{t:.2g}\" for t in ticks])\n",
    "\n",
    "    # plot each point + CI/arrows\n",
    "    for y, row in zip(y_positions, df_fisher.itertuples()):\n",
    "        OR = row.Odds_Ratio\n",
    "        L  = row.Lower_CI\n",
    "        U  = row.Upper_CI\n",
    "        \n",
    "\n",
    "\n",
    "        # clip for plotting point\n",
    "        plot_L = max(L, floor) if L > 0 else floor\n",
    "        if L > OR:\n",
    "            plot_L=floor\n",
    "        plot_U = min(U, cap)   if U < np.inf else cap\n",
    "\n",
    "        # draw main error-bar line (no caps)\n",
    "        ax.plot([plot_L, plot_U], [y, y], color='black', linewidth=1)\n",
    "\n",
    "        # draw the central marker\n",
    "        ax.plot(OR, y, 'o', color='black')\n",
    "\n",
    "        # draw caps or arrows:\n",
    "        #  - if L <= 0, draw left arrow at floor\n",
    "        #print(L)\n",
    "        #print(floor)\n",
    "        #print(plot_L)\n",
    "        if plot_L <= floor:\n",
    "            ax.annotate(\n",
    "                '', xy=(plot_L, y), xytext=(plot_L * (1+arrow_length), y),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color = \"black\")\n",
    "            )\n",
    "        else:\n",
    "            ax.plot(plot_L, y, '|', color='black', markersize=7)\n",
    "\n",
    "        #  - if U == inf, draw right arrow at cap\n",
    "        if U == np.inf:\n",
    "            ax.annotate(\n",
    "                '', xy=(plot_U, y), xytext=(plot_U / (1 + arrow_length), y),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color = \"black\")\n",
    "            )\n",
    "        else:\n",
    "            ax.plot(plot_U, y, '|', color='black', markersize=7)\n",
    "\n",
    "    # labels\n",
    "    plt.yticks(y_positions, df_fisher[\"Test\"] if not custom_labels else custom_labels)\n",
    "    ax.invert_yaxis()\n",
    "    if x_max:\n",
    "        ax.set_xlim(right=x_max)\n",
    "    if x_min:\n",
    "        ax.set_xlim(left=x_min)\n",
    "\n",
    "    plt.xlabel(\"Odds Ratio\")\n",
    "    plt.title(plot_title, fontsize=10, va='bottom', fontweight = 'bold')\n",
    "\n",
    "    # annotations of OR [CI] and p-values, etc\n",
    "    #print(ticks)\n",
    "    OR_text  = x_max * offset_OR\n",
    "    Pval_text = x_max * offset_Pval\n",
    "    header_y = -1\n",
    "    plt.text(OR_text, header_y, \"OR [95% CI]\", ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    plt.text(Pval_text, header_y, \"P-value*\", ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    for y, row in zip(y_positions, df_fisher.itertuples()):\n",
    "        if row.Odds_Ratio == 0:\n",
    "            OR_annotation = f\"{row.Odds_Ratio:.2f} [0-{row.Upper_CI:.2f}]\"\n",
    "        else:\n",
    "            OR_annotation = f\"{row.Odds_Ratio:.2f} [{row.Lower_CI:.2f}-{row.Upper_CI:.2f}]\"\n",
    "        if row.p_value < 0.001:\n",
    "            Pval_annotation = \"<0.001\"\n",
    "        elif row.p_value >= 1:\n",
    "            Pval_annotation = \"1\"\n",
    "        else:\n",
    "            Pval_annotation = f\"{row.p_value:.3f}\"\n",
    "        plt.text(OR_text, y, OR_annotation, va='center', fontsize=9)\n",
    "        plt.text(Pval_text, y, Pval_annotation, va='center', fontsize=9)\n",
    "\n",
    "    # clean up\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(True)\n",
    "    ax.tick_params(axis='y', which='both', left=False, labelleft=True)\n",
    "\n",
    "    \n",
    "    # Set the custom ticks on the x-axis.\n",
    "    # Remove original x-axis ticks\n",
    "    ax.set_xticks([], minor=False)\n",
    "    ax.set_xticks([], minor=True)   \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    \n",
    "   \n",
    "    ymin = -pad\n",
    "    ymax = n - 1 + pad\n",
    "\n",
    "    # because you inverted y, set_ylim(ymax, ymin)\n",
    "    ax.set_ylim(ymax, ymin)\n",
    "    \n",
    "    for tick in plt.gca().yaxis.get_ticklabels():    # Italicize y-axis label\n",
    "        tick.set_fontstyle(\"italic\")\n",
    "    \n",
    "\n",
    "    \n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "    def my_formatter(x, pos):\n",
    "        if x < 1:\n",
    "            # for values less than 1, show up to two decimals,\n",
    "            # but strip trailing zeros (0.50 → .5, 0.25 → .25)\n",
    "            s = f\"{x:.2f}\".rstrip('0').rstrip('.')\n",
    "        else:\n",
    "            # for 1 and above, show no decimals\n",
    "            s = f\"{int(x)}\"\n",
    "        return s\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(my_formatter))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_output_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot_fishertable(df_fisher, plot_title = \"Forest Plot of Fisher Exact Test Results\", \n",
    "                     plot_output_file = \"forest_plot.png\", logbase = 2,figwidth=8,\n",
    "                     custom_labels=False, dpi = 300):\n",
    "    \n",
    "    # Compute the minimum lower CI and maximum upper CI\n",
    "    min_lower = df_fisher[\"Lower_CI\"].min()\n",
    "    max_upper = df_fisher[\"Upper_CI\"].max()\n",
    "\n",
    "    # Determine the exponents for ticks such that:\n",
    "    #   2^(start_exponent) <= min_lower and 2^(end_exponent) > max_upper.\n",
    "    start_exponent = int(np.floor(np.log(min_lower) / np.log(logbase)))\n",
    "    end_exponent = int(np.ceil(np.log(max_upper) / np.log(logbase))) + 1\n",
    "    print(start_exponent)\n",
    "\n",
    "    # Generate tick positions on a power-of-2 scale.\n",
    "    ticks = np.logspace(start_exponent, end_exponent, num=end_exponent - start_exponent + 1, base=logbase)\n",
    "    # Create tick labels directly from ticks (convert to int if desired, or leave as float)\n",
    "    tick_labels = [str((t)) for t in ticks]\n",
    "    \n",
    "    #find number of tests\n",
    "    n_tests = df_fisher.shape[0]\n",
    "    y_positions = np.arange(n_tests)\n",
    "\n",
    "\n",
    "    # Plot a forest plot-like graph\n",
    "    plt.figure(figsize=(figwidth, n_tests * 0.5))\n",
    "    ax = plt.gca()  # get current axis\n",
    "    \n",
    "    # Plot the odds ratio with error bars representing the 95% CI (Bonferroni corrected)\n",
    "    plt.errorbar(df_fisher[\"Odds_Ratio\"], y_positions,\n",
    "                 xerr=[df_fisher[\"Odds_Ratio\"] - df_fisher[\"Lower_CI\"],\n",
    "                       df_fisher[\"Upper_CI\"] - df_fisher[\"Odds_Ratio\"]],\n",
    "                 fmt='o', color='black', capsize=5)\n",
    "\n",
    "\n",
    "    # Set the x-axis to logarithmic scale\n",
    "    plt.xscale('log')\n",
    "\n",
    "    # Draw a vertical line at an odds ratio of 1 (no effect)\n",
    "    plt.axvline(x=1, color='red', linestyle='--')\n",
    "\n",
    "    # Label the y-axis with the test names\n",
    "    plt.yticks(y_positions, df_fisher[\"Test\"])\n",
    "    if(custom_labels):\n",
    "        plt.yticks(y_positions, custom_labels)\n",
    "\n",
    "    # Invert the y-axis so that \"Test 1\" appears at the top.\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.xlabel(\"Odds Ratio\")\n",
    "    plt.title(plot_title, fontsize = 12, va = 'bottom')\n",
    "\n",
    "    # Determine a right-hand margin for annotations.\n",
    "    # We take the maximum upper CI value and extend the x-axis a bit.\n",
    "    max_upper = df_fisher[\"Upper_CI\"].max()\n",
    "    OR_text = max(ticks) * 1.4\n",
    "    Pval_text = max(ticks) * 7.5\n",
    "    #plt.xlim(left=0.1, right=Pval_text * 0.9)\n",
    "\n",
    "    # Add header annotation above the individual annotations\n",
    "    header_y = n_tests \n",
    "    header_y = -1\n",
    "    plt.text(OR_text, header_y, \"OR [95% CI]\", ha='left', va='top', fontsize=10, fontweight='bold')\n",
    "    plt.text(Pval_text, header_y, \"P-value*\", ha='left', va='top', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Annotate each point with the odds ratio and its Bonferroni-corrected CI\n",
    "    for y, row in zip(y_positions, df_fisher.itertuples()):\n",
    "        OR_annotation = f\"{row.Odds_Ratio:.2f} [{row.Lower_CI:.2f}-{row.Upper_CI:.2f}]\"\n",
    "        if row.p_value < 0.001:\n",
    "            Pval_annotation = \"<0.001\"\n",
    "        elif row.p_value >=1:\n",
    "            Pval_annotation = \"1\"\n",
    "        else:\n",
    "            Pval_annotation = f\"{row.p_value:.3f}\"\n",
    "        plt.text(OR_text, y, OR_annotation, va='center', fontsize=9)\n",
    "        plt.text(Pval_text, y, Pval_annotation, va = 'center', fontsize = 9)\n",
    "\n",
    "    # Remove the left and right spines of the plot so they don't overlap with the annotations.\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(True)\n",
    "\n",
    "    # Remove y-axis ticks and labels\n",
    "    ax.tick_params(axis='y', which='both', left=False, labelleft=True)\n",
    "    \n",
    "    # Set the custom ticks on the x-axis.\n",
    "    # Remove original x-axis ticks\n",
    "    ax.set_xticks([], minor=False)\n",
    "    ax.set_xticks([], minor=True)   \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_output_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot_stacked_bars(sol_table_summ, solution_category,plot_title, save_path, fig_width = 7, fig_height = 7, offset = 0.3):\n",
    "    \n",
    "    #sol_table = sol_table_summ[sol_table_summ[\"category\"].str.contains(solution_category)]\n",
    "    sol_table = sol_table_summ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #starting table\n",
    "    con_sol_table = sol_table[sol_table.solution_table == solution_category] #plot AD\n",
    "\n",
    "    #concept categories in selected table\n",
    "    concept_categories = con_sol_table[\"category\"].unique()\n",
    "    print(concept_categories)\n",
    "    \n",
    "    # Sort data by person_count for better visualization\n",
    "    sol_table = sol_table.sort_values(\"person_count_solution\", ascending=False).reset_index()\n",
    "    \n",
    "    #display(sol_table)\n",
    "    genes_to_plot = sol_table[\"gene\"].unique()\n",
    "    \n",
    "\n",
    "\n",
    "    y_positions = np.arange(0, len(genes_to_plot)/2, 0.5)\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    #ax = fig.add_axes([0.2, 0.1, 0.7, 0.8])\n",
    "\n",
    "    bar_height = 0.4  # Height of each bar\n",
    "\n",
    "    #first by gene\n",
    "    for i, gene_to_plot in enumerate(genes_to_plot):\n",
    "\n",
    "        #print(\"Processing gene:\", gene_to_plot)\n",
    "        #print(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize an empty list for person_annotation_rates\n",
    "        x_abs = []\n",
    "\n",
    "        # Filter the main table for the current gene\n",
    "        con_sol_table_gene = con_sol_table[con_sol_table['gene'] == gene_to_plot]\n",
    "        if not con_sol_table_gene.empty:\n",
    "            # proceed if there is an entry for the gene in the sol x concept table (i.e. at least 1 person annotated)\n",
    "\n",
    "            # Iterate over the unique concept categories\n",
    "            for k, concept_category in enumerate(concept_categories):\n",
    "                #print(\"Concept Category:\", concept_category)\n",
    "\n",
    "                # Filter for the current concept category within the gene-specific table\n",
    "                subset = con_sol_table_gene[con_sol_table_gene['category'] == concept_category]\n",
    "\n",
    "                if not subset.empty:\n",
    "                    # Extract person_annotation_rate from the first matching row (adjust if necessary)\n",
    "                    rate = subset.iloc[0]['person_annotation_rate']*100\n",
    "                    x_abs.append(rate)\n",
    "                else:\n",
    "                    # If the category is not present, check if it's the first element\n",
    "                    if k == 0:\n",
    "                        x_abs.append(0)\n",
    "                    else:\n",
    "                        # Use the previous rate if available\n",
    "                        x_abs.append(x_abs[k-1])\n",
    "        else:\n",
    "            #if not present, set all to 0\n",
    "            x_abs = [0,0,0]\n",
    "\n",
    "        x_delta = [x_abs[0]] + [x_abs[i] - x_abs[i - 1] for i in range(1, len(x_abs))]\n",
    "\n",
    "        y = y_positions[i]\n",
    "        left = 0  # starting position for the first segment\n",
    "        for j in range(len(x_delta)):\n",
    "            ax.barh(y, x_delta[j],\n",
    "                    left=left,\n",
    "                    height=bar_height,\n",
    "                    color=colors_concepts[j],\n",
    "                    #align='edge',\n",
    "                    edgecolor='none',   # Remove the edge (border) to eliminate white gaps\n",
    "                    linewidth=0)        # Alternatively, set linewidth to 0\n",
    "            left += x_delta[j]\n",
    "\n",
    "    # Customize the y-axis to show group names at the center of each bar\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(genes_to_plot)\n",
    "    ax.set_ylim(-offset, max(y_positions)+offset)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    ax.set_xlim(0, 100)\n",
    "    #ax.set_xlabel(\"Disease Annotation Frequency\")\n",
    "    plt.gca().set_xlabel('Disease Annotation Frequency (%)', fontdict=dict(weight='bold'))\n",
    "    #ax.set_aspect('equal')  # or use a numeric value: ax.set_aspect(1)\n",
    "\n",
    "    \n",
    "    #draw con_prev_lines\n",
    "    plt.axvline(x=con_prev[0]*100, color='grey', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=con_prev[1]*100, color='grey', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=con_prev[2]*100, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(\n",
    "        plot_title,\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"center\"\n",
    "    )\n",
    "    \n",
    "    # Create custom legend patches\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=colors_concepts[0], label=\"IRD\"),\n",
    "        mpatches.Patch(color=colors_concepts[1], label=\"Retinal Degeneration\"),\n",
    "        mpatches.Patch(color=colors_concepts[2], label=\"Screening Set\"),\n",
    "    ]\n",
    "    #ax.legend(handles=legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    for tick in plt.gca().yaxis.get_ticklabels():    # Italicize y-axis label\n",
    "        tick.set_fontstyle(\"italic\")\n",
    "\n",
    "\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    #plt.subplots_adjust(top=0.95, bottom=0.05)\n",
    "\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    plt.rcParams['figure.constrained_layout.use'] = True\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_stacked_bars_bygene(sol_table_summ, \n",
    "                             solution_category,plot_title, \n",
    "                             save_path, fig_width = 7, fig_height = 7, offset = 0.3):\n",
    "    \n",
    "    #sol_table = sol_table_summ[sol_table_summ[\"category\"].str.contains(solution_category)]\n",
    "    sol_table = sol_table_summ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #starting table\n",
    "    con_sol_table = sol_table[sol_table.solution_table == solution_category] #plot AD\n",
    "\n",
    "    #concept categories in selected table\n",
    "    concept_categories = con_sol_table[\"category\"].unique()\n",
    "    print(concept_categories)\n",
    "    \n",
    "    # Sort data by person_count for better visualization\n",
    "    sol_table = sol_table.sort_values(\"person_annotation_rate\", ascending=False).reset_index()\n",
    "    \n",
    "    #display(sol_table)\n",
    "    genes_to_plot = sol_table[\"gene\"].unique()\n",
    "    \n",
    "\n",
    "\n",
    "    y_positions = np.arange(0, len(genes_to_plot)/2, 0.5)\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    #ax = fig.add_axes([0.2, 0.1, 0.7, 0.8])\n",
    "\n",
    "    bar_height = 0.4  # Height of each bar\n",
    "\n",
    "    #first by gene\n",
    "    for i, gene_to_plot in enumerate(genes_to_plot):\n",
    "\n",
    "        #print(\"Processing gene:\", gene_to_plot)\n",
    "        #print(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize an empty list for person_annotation_rates\n",
    "        x_abs = []\n",
    "\n",
    "        # Filter the main table for the current gene\n",
    "        con_sol_table_gene = con_sol_table[con_sol_table['gene'] == gene_to_plot]\n",
    "        if not con_sol_table_gene.empty:\n",
    "            # proceed if there is an entry for the gene in the sol x concept table (i.e. at least 1 person annotated)\n",
    "\n",
    "            # Iterate over the unique concept categories\n",
    "            for k, concept_category in enumerate(concept_categories):\n",
    "                #print(\"Concept Category:\", concept_category)\n",
    "\n",
    "                # Filter for the current concept category within the gene-specific table\n",
    "                subset = con_sol_table_gene[con_sol_table_gene['category'] == concept_category]\n",
    "\n",
    "                if not subset.empty:\n",
    "                    # Extract person_annotation_rate from the first matching row (adjust if necessary)\n",
    "                    rate = subset.iloc[0]['person_annotation_rate'] * 100\n",
    "                    x_abs.append(rate)\n",
    "                else:\n",
    "                    # If the category is not present, check if it's the first element\n",
    "                    if k == 0:\n",
    "                        x_abs.append(0)\n",
    "                    else:\n",
    "                        # Use the previous rate if available\n",
    "                        x_abs.append(x_abs[k-1])\n",
    "        else:\n",
    "            #if not present, set all to 0\n",
    "            x_abs = [0,0,0]\n",
    "\n",
    "        x_delta = [x_abs[0]] + [x_abs[i] - x_abs[i - 1] for i in range(1, len(x_abs))]\n",
    "\n",
    "        y = y_positions[i]\n",
    "        left = 0  # starting position for the first segment\n",
    "        for j in range(len(x_delta)):\n",
    "            ax.barh(y, x_delta[j],\n",
    "                    left=left,\n",
    "                    height=bar_height,\n",
    "                    color=colors_concepts[j],\n",
    "                    #align='edge',\n",
    "                    edgecolor='none',   # Remove the edge (border) to eliminate white gaps\n",
    "                    linewidth=0)        # Alternatively, set linewidth to 0\n",
    "            left += x_delta[j]\n",
    "\n",
    "    # Customize the y-axis to show group names at the center of each bar\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(genes_to_plot)\n",
    "    ax.set_ylim(-offset, max(y_positions)+offset)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    ax.set_xlim(0, 100)\n",
    "    #ax.set_xlabel(\"Disease Annotation Frequency\")\n",
    "    plt.gca().set_xlabel('Disease Annotation Frequency (%)', fontdict=dict(weight='bold'))\n",
    "    #ax.set_aspect('equal')  # or use a numeric value: ax.set_aspect(1)\n",
    "\n",
    "    \n",
    "    #draw con_prev_lines\n",
    "    plt.axvline(x=con_prev[0]*100, color='grey', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=con_prev[1]*100, color='grey', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=con_prev[2]*100, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(\n",
    "        plot_title,\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"center\"\n",
    "    )\n",
    "    \n",
    "    # Create custom legend patches\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=colors_concepts[0], label=\"IRD\"),\n",
    "        mpatches.Patch(color=colors_concepts[1], label=\"Retinal Degeneration\"),\n",
    "        mpatches.Patch(color=colors_concepts[2], label=\"Screening Set\"),\n",
    "    ]\n",
    "    #ax.legend(handles=legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    for tick in plt.gca().yaxis.get_ticklabels():    # Italicize y-axis label\n",
    "        tick.set_fontstyle(\"italic\")\n",
    "\n",
    "\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    #plt.subplots_adjust(top=0.95, bottom=0.05)\n",
    "\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    plt.rcParams['figure.constrained_layout.use'] = True\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Derive colors for the stacked bar plot\n",
    "# Define the nested set sizes (cumulative counts)\n",
    "set_sizes = [1060, 14765, 52836, 115412, 317964]\n",
    "\n",
    "# Compute segments representing the additional entries in each nested set.\n",
    "segments = [\n",
    "    set_sizes[0],\n",
    "    set_sizes[1] - set_sizes[0],\n",
    "    set_sizes[2] - set_sizes[1],\n",
    "    set_sizes[3] - set_sizes[2],\n",
    "    set_sizes[4] - set_sizes[3]\n",
    "]\n",
    "\n",
    "total = set_sizes[-1]\n",
    "\n",
    "# Calculate percentages relative to the largest set.\n",
    "pwr = 1/1.5\n",
    "percentages = [(seg / total)**pwr for seg in set_sizes]\n",
    "\n",
    "# Define labels for each set.\n",
    "\n",
    "\n",
    "# Choose colors from a colormap.\n",
    "colors = plt.cm.Blues(percentages[::-1])\n",
    "\n",
    "\n",
    "#colors_concepts = [\"darkblue\", \"blue\", \"lightblue\"] #IRD, RetDegen, Screen\n",
    "colors_concepts = colors[0:3]\n",
    "\n",
    "\n",
    "def plot_gene_summary_aggregated(solution_gene_summary, \n",
    "                                 category = \"retdys\", \n",
    "                                 title=\"Gene Summary\", \n",
    "                                 save_path=None, \n",
    "                                 height=8, \n",
    "                                 width=7, \n",
    "                                 offset = 2):\n",
    "    \"\"\"\n",
    "    Create a barplot for a specific category of genes in solution_gene_summary.\n",
    "\n",
    "    Parameters:\n",
    "    - solution_gene_summary (pd.DataFrame): DataFrame containing gene summaries.\n",
    "    - category (str): Category to filter the data.\n",
    "    - title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # Filter the data for the specified category\n",
    "    category_q = f\"^{category}$\"\n",
    "    \n",
    "    category_data = solution_gene_summary[solution_gene_summary[\"category\"].str.contains(category_q)]\n",
    "\n",
    "    if category_data.empty:\n",
    "        print(f\"No data available for category: {category}\")\n",
    "        return\n",
    "\n",
    "    # Sort data by person_count for better visualization\n",
    "    category_data = category_data.sort_values(\"person_count_solution\", ascending=False).reset_index()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(width, height))\n",
    "    sns.barplot(\n",
    "        data=category_data,\n",
    "        y=\"gene\",  # Genes on the y-axis for a horizontal bar plot\n",
    "        x=\"person_count_solution\",\n",
    "        color=\"lightgrey\"\n",
    "    )\n",
    "    \n",
    "    # Barplot for variant_count (overlaid with transparency)\n",
    "    sns.barplot(\n",
    "        data=category_data,\n",
    "        y=\"gene\",\n",
    "        x=\"variant_count_solution\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,  # Make the bars partially transparent\n",
    "        label=\"Variant Count\"  # Add label for the legend\n",
    "    )\n",
    "\n",
    "    # Add annotations for person_count\n",
    "    for index, row in category_data.iterrows():\n",
    "        plt.text(\n",
    "            row[\"person_count_solution\"] + offset,  # Offset to the right of the bar\n",
    "            index,  # Position along the y-axis\n",
    "            f'{int(row[\"person_count_solution\"])}',  # Annotation text\n",
    "            va=\"center\",\n",
    "            fontsize = 10\n",
    "        )\n",
    "\n",
    "    # Add labels and title\n",
    "    #plt.title(\n",
    "    #    title,\n",
    "    #    fontsize=16,\n",
    "    #    fontweight=\"bold\",\n",
    "    #    ha=\"center\"\n",
    "    #)\n",
    "    #plt.xlabel(\"Number of Participants\")\n",
    "    #plt.ylabel(\"Gene\")\n",
    "    plt.gca().yaxis.set_tick_params(labelsize=10)\n",
    "    #plt.gca().yaxis.get_label().set_fontstyle(\"italic\")  \n",
    "    plt.gca().set_xlabel('Number of Participants', fontdict=dict(weight='bold'))\n",
    "    plt.gca().set_ylabel('Gene', fontdict=dict(weight='bold'))\n",
    "    plt.grid(axis='x', visible=False)\n",
    "    for tick in plt.gca().yaxis.get_ticklabels():    # Italicize y-axis label\n",
    "        tick.set_fontstyle(\"italic\")\n",
    "    plt.gca().xaxis.grid(False)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    \n",
    "    sns.despine()\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "    \n",
    "    #plt.grid(b=None)\n",
    "    # Save the plot if a save_path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    # Adjust layout\n",
    "    #plt.grid(None)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def procRace(df):\n",
    "    df.loc[df['race_source_value'].isin([\"AoUDRC_NoneIndicated\",\n",
    "                                   \"PMI_Skip\",\n",
    "                                   \"PMI_PreferNotToAnswer\"]), 'race_source_value'] = \"Unknown\"\n",
    "\n",
    "\n",
    "    df.loc[df['race_source_value'].isin([\"WhatRaceEthnicity_GeneralizedMultPopulations\",\n",
    "                                   \"WhatRaceEthnicity_RaceEthnicityNoneOfThese\"]), 'race_source_value'] = \"Other\"\n",
    "    df = df.rename(columns={'race_source_value': 'race'})\n",
    "\n",
    "    return(df)\n",
    "\n",
    "def replaceRace(df):\n",
    "    df['race'] = df['race'].replace({\n",
    "    'WhatRaceEthnicity_White': 'White',\n",
    "    'WhatRaceEthnicity_Black':   'Black',\n",
    "    'WhatRaceEthnicity_Asian': 'Asian',\n",
    "    'WhatRaceEthnicity_AIAN':   'AIAN',\n",
    "    'WhatRaceEthnicity_MENA': 'MENA'\n",
    "    })\n",
    "    \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebf770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "# Set global parameters for a Tufte-like minimalist look\n",
    "plt.rcParams.update({\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.edgecolor': 'gray',\n",
    "    'axes.linewidth': 0.8,\n",
    "    'grid.linestyle': '--',\n",
    "    'grid.linewidth': 0.5,\n",
    "    'grid.color': 'gray',\n",
    "    'xtick.top': False,\n",
    "    'ytick.right': False,\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif' : ['Arial', 'Tahoma', 'DejaVu Sans',\n",
    "                               'Lucida Grande', 'Verdana'],\n",
    "    'font.size': 12,\n",
    "    # Remove background color and excessive ticks\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white'\n",
    "})\n",
    "\n",
    "matplotlib.rc('font', size=12)          # Set default text size\n",
    "matplotlib.rc('axes', titlesize=14)       # Set axes title font size\n",
    "matplotlib.rc('axes', labelsize=12)       # Set label font size\n",
    "matplotlib.rc('xtick', labelsize=10)      # Set x tick label size\n",
    "matplotlib.rc('ytick', labelsize=10)      # Set y tick label size\n",
    "matplotlib.rc('legend', fontsize=10)      # Set legend font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load solution tables (personIDs with IRD-comopatible genotypes & associated metadata)\n",
    "AD_subset = getTable('GS_AD_solutions.tsv',\"gene_solutions\")\n",
    "XL_subset_male = getTable('GS_XLmale_solutions.tsv',\"gene_solutions\")\n",
    "Hom_subset = getTable(\"pid_vid_hom_noXLmale.tsv\", \"personID_variant\")\n",
    "ALL_IRD = getTable(\"ALL_IRD.tsv\", \"personID_concept\") #if you have already done the above\n",
    "# Dictionary of solution tables - names must match!\n",
    "solution_tables = {\n",
    "    \"ALLIRD_solutions\" : ALL_IRD,\n",
    "    #\"AD_solutions\": AD_subset,\n",
    "    #\"XLmale_solutions\": XL_subset_male,\n",
    "    #\"Hom_solutions\" : Hom_subset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_inheritance_summary = describe_solution_tables(**solution_tables)\n",
    "saveToBucket(solution_inheritance_summary, \"solution_inheritance_summary.tsv\", \"Concept_Solution\")\n",
    "solution_inheritance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get concept_person tables (personIDs matching ICD code sets & associated metadata)\n",
    "retdys_concept_person = getTable(table_name = \"retchordys_ICD_person_age_youngest.tsv\", folder = \"personID_concept\")\n",
    "retdegen_concept_person = getTable(table_name = \"retdegen_ICD_person_age_youngest.tsv\", folder = \"personID_concept\")\n",
    "screenretdys_concept_person = getTable(table_name = \"ScreenRetDysICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "AMDspecific_concept_person = getTable(table_name = \"AMD_specific_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "AMDexudative_concept_person = getTable(table_name = \"AMD_exudative_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "AMDnonexudative_concept_person = getTable(table_name = \"AMD_nonexudative_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "CME_concept_person = getTable(table_name = \"CME_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "\n",
    "myopia_concept_person = getTable(table_name = \"myopia_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "hypermetropia_concept_person = getTable(table_name = \"hypermetropia_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")\n",
    "pucker_concept_person = getTable(table_name = \"pucker_ICD_person_age_youngest.tsv\",  folder = \"personID_concept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30251c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe concept tables\n",
    "concept_tables = {\n",
    "    \"retdys_concept_person\" : retdys_concept_person,\n",
    "    \"retdegen_concept_person\" : retdegen_concept_person,\n",
    "    \"screenretdys_concept_person\" : screenretdys_concept_person,\n",
    "    #\"AMDexudative_concept_person\" : AMDexudative_concept_person,\n",
    "    #\"AMDnonexudative_concept_person\" : AMDnonexudative_concept_person,\n",
    "    \"AMDspecific_concept_person\" : AMDspecific_concept_person,\n",
    "    \"CME_concept_person\" : CME_concept_person,\n",
    "    \"pucker_concept_person\" : pucker_concept_person,\n",
    "    \"myopia_concept_person\" : myopia_concept_person,\n",
    "    \"hypermetropia_concept_person\" : hypermetropia_concept_person\n",
    "    \n",
    "}\n",
    "\n",
    "concept_summary = describe_tables(**concept_tables)\n",
    "#saveToBucket(concept_summary, \"concept_summary.tsv\", \"Concept_Solution\")\n",
    "concept_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f647b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries from solution and concept tables\n",
    "solution_gene_summary = process_solution_gene_summaries(solution_tables)\n",
    "concept_gene_summary = process_concept_solution_gene_summaries(concept_tables, solution_tables)\n",
    "    \n",
    "# Calculate annotation rates\n",
    "annotation_rates_solution_gene = calculate_all_annotation_rates(solution_gene_summary, concept_gene_summary)\n",
    "\n",
    "#saveToBucket(annotation_rates_solution_gene, \"annotation_rates_solution_GS.tsv\", \"Concept_Solution\")\n",
    "#saveToBucket(solution_gene_summary, \"solution_geneGS_summary.tsv\", \"Concept_Solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c148576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate all genes with 20 or less participants\n",
    "\n",
    "full_table = pd.DataFrame()\n",
    "table_subset = pd.DataFrame()\n",
    "for table in annotation_rates_solution_gene.table.unique():\n",
    "    table_subset = annotation_rates_solution_gene[annotation_rates_solution_gene[\"table\"] == table]\n",
    "    mask = table_subset['person_count_solution'] <= 20\n",
    "\n",
    "    # sum up the “ultra-rare” group at person-level\n",
    "    sum_person_count = table_subset.loc[mask, 'person_count_solution'].sum()\n",
    "    sum_person_concept = table_subset.loc[mask, 'person_count_concept'].sum()\n",
    "    \n",
    "    # sum up the \"ultra-rare\" group at variant-level\n",
    "    sum_variant_count = table_subset.loc[mask, 'variant_count_solution'].sum()\n",
    "    sum_variant_concept = table_subset.loc[mask, 'variant_count_concept'].sum()\n",
    "    \n",
    "    \n",
    "    # build the new row\n",
    "    new_row = {\n",
    "        'concept': table_subset.concept.unique()[0],\n",
    "        'table': table_subset.table.unique()[0],\n",
    "        'solution_table': table_subset.solution_table.unique()[0],\n",
    "        'category': table_subset.category.unique()[0],\n",
    "        'category_inheritance': table_subset.category_inheritance.unique()[0],\n",
    "        'gene': 'Ultra-rare',\n",
    "        'person_count_solution': sum_person_count,\n",
    "        'person_count_concept': sum_person_concept,\n",
    "        'person_annotation_rate': sum_person_concept / sum_person_count if sum_person_concept else float('nan'),\n",
    "        'variant_count_solution' : sum_variant_count,\n",
    "        'variant_count_concept' : sum_variant_concept,\n",
    "        'variant_annotation_rate' : sum_variant_concept / sum_variant_count if sum_variant_concept else float('nan')\n",
    "    }\n",
    "    \n",
    "    # drop the old “ultra-rare” rows and append the aggregate\n",
    "    table_subset = pd.concat([\n",
    "        table_subset.loc[~mask],        # keep only rows where col3 > 20\n",
    "        pd.DataFrame([new_row])         # add aggregated row\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    full_table = pd.concat([full_table, table_subset], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get enrichment/forest plots for code set by gene\n",
    "# change concept as desired to get plot (retdys_concept_person = IRD set, retdegen_concept_person = Retinopathy set, etc.)\n",
    "# change annotation_rates_solution_gene to the table you want to plot (full_table for aggregated data, raw annotation_rates_solution_gene for all genes)\n",
    "a = loop_through_fisher_bygene(annotation_rates_solution_gene, concept = \"retdys_concept_person\")\n",
    "plot_fishertable_with_arrows(a, plot_title = \"Enrichment of IRD Set\",\n",
    "                plot_output_file = \"forest_plot_gene_in_IRD_bygene.png\", logbase = 2, \n",
    "                dpi=300, offset_OR = 1.5, offset_Pval = 8, fig_width = 10, fig_height = 10,\n",
    "                            x_max=128, x_min=0.25, pad = 0.35, cap = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd13386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrichment plot for the entire dataset, as see in Figure 1 of the paper\n",
    "custom_labels = [\"IRD\",\n",
    "                \"Retinopathy\",\n",
    "                \"Screening Set\",\n",
    "                \"AMD\",\n",
    "                \"CME\",\n",
    "                \"Pucker / ERM\",\n",
    "                \"Myopia\",\n",
    "                \"Hypermetropia\"\n",
    "                ]\n",
    "\n",
    "plot_fisher_tbl = loop_through_fisher(anno_rates_table = annotation_rates_solution_gene)\n",
    "plot_fishertable(plot_fisher_tbl, plot_title = \"Enrichment within IRD genotype\",\n",
    "                plot_output_file = \"forest_plot_IRD_vs_concept.png\", logbase = 2, custom_labels = custom_labels,\n",
    "                 figwidth=9,\n",
    "                dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a626c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap as seen in Figure 1\n",
    "# Extract the heatmap data from the two value columns\n",
    "heatmap_data = plot_fisher_tbl[['Concept_prevalence', 'DAR']].values * 100\n",
    "\n",
    "n_tests = plot_fisher_tbl.shape[0]\n",
    "\n",
    "# Create the heatmap plot using imshow\n",
    "fig, ax = plt.subplots(figsize=(4, n_tests*0.5))\n",
    "cax = ax.imshow(heatmap_data, aspect='auto', cmap='OrRd', vmax=40, origin='upper')\n",
    "\n",
    "# Set row labels using the Title column from the DataFrame\n",
    "ax.set_yticks(np.arange(len(plot_fisher_tbl)))\n",
    "ax.set_yticklabels(custom_labels)\n",
    "\n",
    "# Optionally, set column labels for the two value columns\n",
    "ax.set_xticks(np.arange(heatmap_data.shape[1]))\n",
    "ax.set_xticklabels(['Prevalence', 'DAF'])\n",
    "\n",
    "# Annotate each cell with its data value\n",
    "for i in range(heatmap_data.shape[0]):\n",
    "    for j in range(heatmap_data.shape[1]):\n",
    "        # The format can be adjusted based on your needs\n",
    "        ax.text(j, i, f'{heatmap_data[i, j]:.2f}', ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "# Add a title to the plot and a colorbar\n",
    "plt.title('AoU Prevalence \\n and DAF (%)')\n",
    "#plt.colorbar(cax)\n",
    "\n",
    "#Remove spines\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ConPrev_andAnnoRate.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart as seen in Figure 1\n",
    "#data = solution_inheritance_summary[\"count\"]\n",
    "data = [374, 76, 31]\n",
    "\n",
    "# pick a colormap and sample N colors from it\n",
    "cmap = plt.get_cmap('Set3')       \n",
    "#colors = cmap(np.linspace(0, 1, len(data)))\n",
    "colors = cmap.colors[:len(data)]\n",
    "\n",
    "\n",
    "#labels = solution_inheritance_summary.Table_Name\n",
    "labels = [\"Autosomal Dominant \\n (N=374)\", \" X-linked Male or \\n Homozygous Female\\n (N=76)\", \n",
    "          \"Homozygous \\nAutosomal Recessive \\n (N=31)\"]\n",
    "\n",
    "# Create the pie chart with percentage labels\n",
    "plt.pie(data, labels=labels, colors = colors, autopct='%1.1f%%')\n",
    "plt.title('Participants with definite \\n IRD-compatible genotypes \\n N=481')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"participant_dist.png\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d64c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of # of participants per gene in the set as seen in Figure 2\n",
    "# Assuming solution_gene_summary is a DataFrame with columns: \"category\", \"gene\", \"person_count\"\n",
    "plot_gene_summary_aggregated(\n",
    "    solution_gene_summary=full_table,\n",
    "    category=\"retdys\",  # unnecessary for this plot, but can be used to filter by category\n",
    "    title=\"Number of Participants\",\n",
    "    height = 4,\n",
    "    width = 5,\n",
    "    save_path=\"ALL_Solutions_var.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f46342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAFs per gene in the set as seen in Figure 2\n",
    "\n",
    "#get solution and concept categories for to setting up looping or indexing below\n",
    "solution_categories = core_concept_anno_gene[\"solution_table\"].unique()\n",
    "concept_categories = core_concept_anno_gene[\"category\"].unique()\n",
    "\n",
    "\n",
    "# Make the stacked bar plot - change the solution_category to plot different sets\n",
    "plot_stacked_bars(sol_table_summ = core_concept_anno_aggregated,\n",
    "                  solution_category=solution_categories[0],\n",
    "                  plot_title=\"\",\n",
    "                  save_path = \"anno_rate_ALL.png\",\n",
    "                  fig_width=6,\n",
    "                  fig_height=4,\n",
    "                 offset = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age plot as seen in Figure 4\n",
    "\n",
    "an_mask = ALLIRD_in_retdys.condition_source_value.isnull() # or df.isna()\n",
    "in_retdys = ALLIRD_in_retdys[~nan_mask]\n",
    "notin_retdys = ALL_IRD[nan_mask]\n",
    "\n",
    "\n",
    "nan_mask = ALLIRD_in_retdegen.condition_source_value.isnull() # or df.isna()\n",
    "in_retdegen = ALLIRD_in_retdegen[~nan_mask]\n",
    "notin_retdegen = ALL_IRD[nan_mask]\n",
    "\n",
    "\n",
    "nan_mask = ALLIRD_in_screenretdys.condition_source_value.isnull() # or df.isna()\n",
    "in_screenretdys = ALLIRD_in_screenretdys[~nan_mask]\n",
    "notin_screenretdys = ALL_IRD[nan_mask]\n",
    "\n",
    "annotationRate_tables = {\n",
    "    \"in_retdys\" : in_retdys,\n",
    "    \"in_retdegen\" :in_retdegen,\n",
    "    \"in_screenretdys\": in_screenretdys\n",
    "}\n",
    "\n",
    "Unannotated_tables = {\n",
    "    \"notin_retdys\" : notin_retdys,\n",
    "    \"notin_retdegen\" :notin_retdegen,\n",
    "    \"notin_screenretdys\": notin_screenretdys\n",
    "}\n",
    "\n",
    "#violin plots\n",
    "in_retdys_sm = in_retdys[[\"AgeAtVisit_Years\"]]\n",
    "in_retdys_sm[\"set\"] = \"IRD\"\n",
    "in_retdys_sm = in_retdys_sm.rename(columns={'AgeAtVisit_Years': 'Age'})\n",
    "\n",
    "\n",
    "in_retdegen_sm = in_retdegen[[\"AgeAtVisit_Years\"]]\n",
    "in_retdegen_sm[\"set\"] = \"Retinal Degeneration\"\n",
    "in_retdegen_sm = in_retdegen_sm.rename(columns={'AgeAtVisit_Years': 'Age'})\n",
    "\n",
    "in_screenretdys_sm = in_screenretdys[[\"AgeAtVisit_Years\"]]\n",
    "in_screenretdys_sm[\"set\"] = \"Screening Set\"\n",
    "in_screenretdys_sm = in_screenretdys_sm.rename(columns={'AgeAtVisit_Years': 'Age'})\n",
    "\n",
    "ALL_IRD_sm = ALL_IRD[[\"AgeAtConsent_Years\"]]\n",
    "ALL_IRD_sm[\"set\"] = \"All Participants\"\n",
    "ALL_IRD_sm = ALL_IRD.rename(columns={'AgeAtConsent_Years': 'Age'})\n",
    "\n",
    "notin_retdys_sm = notin_retdys[[\"AgeAtConsent_Years\"]]\n",
    "notin_retdys_sm[\"set\"] = \"Not in IRD\"\n",
    "notin_retdys_sm = notin_retdys_sm.rename(columns={'AgeAtConsent_Years': 'Age'})\n",
    "\n",
    "\n",
    "notin_retdegen_sm = notin_retdegen[[\"AgeAtConsent_Years\"]]\n",
    "notin_retdegen_sm[\"set\"] = \"Not in Retinal Degeneration\"\n",
    "notin_retdegen_sm = notin_retdegen_sm.rename(columns={'AgeAtConsent_Years': 'Age'})\n",
    "\n",
    "notin_screenretdys_sm = notin_screenretdys[[\"AgeAtConsent_Years\"]]\n",
    "notin_screenretdys_sm[\"set\"] = \"Not in Screening Set\"\n",
    "notin_screenretdys_sm = notin_screenretdys_sm.rename(columns={'AgeAtConsent_Years': 'Age'})\n",
    "\n",
    "df = pd.concat([in_retdys_sm, notin_retdys_sm, \n",
    "                in_retdegen_sm, notin_retdegen_sm, \n",
    "                in_screenretdys_sm, notin_screenretdys_sm])\n",
    "\n",
    "palette = {\n",
    "    'IRD':        '#0F5786',\n",
    "    'Not in IRD':         'white',\n",
    "    'Retinal Degeneration': '#58b1db',\n",
    "    'Not in Retinal Degeneration':  'white',\n",
    "    'Screening Set' : '#b0d2e7',\n",
    "    'Not in Screening Set' : \"white\"\n",
    "    # … add entries for every Code Set …\n",
    "}\n",
    "\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "df = df.dropna(subset=['Age'])\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "#fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "ax = fig.add_axes([0.25, 0.15, 0.70,0.80])\n",
    "ax = sns.violinplot(\n",
    "    data=df,\n",
    "    x='set',\n",
    "    y='Age',\n",
    "    #hue='Annotated',\n",
    "    split=True,\n",
    "    #inner=\"box\",\n",
    "    #common_norm = True,\n",
    "    saturation = 1,\n",
    "    inner_kws=dict(box_width=30, whis_width=2, color=\".8\"),\n",
    "    density_norm=\"count\",\n",
    "\n",
    "    \n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(['IRD', 'Not in\\nIRD', \n",
    "                    'Retinopathy', 'Not in\\nRetinopathy', \n",
    "                    'Screening\\nSet', 'Not in\\nScreening\\nSet'])\n",
    "\n",
    "ax.set(xlabel=\"Code Set Annotation\")\n",
    "ax.set_ylim(-10, 110)\n",
    "\n",
    "\n",
    "\n",
    "#sns.swarmplot(x ='set', y ='Age', data = df,color= \"white\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title('AoU Age Distribution')\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"AoUAgeDist.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff82394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UKBB age distribution plot as see in Figure 4\n",
    "\n",
    "UKBB_data_images = getTable(\"UKBB_data_images.tsv\", \"UKBB_data\")\n",
    "df = UKBB_data_images\n",
    "#df = df.sort()\n",
    "\n",
    "colors_UKBB = [\"#9bd08a\", \"white\"]\n",
    "colors_UKBB2 = [\"#116434\", \"#9bd08a\", \"white\"]\n",
    "\n",
    "df.Phenotype = pd.Categorical(df.Phenotype, \n",
    "                      categories=[\"All Abnormal\",\"Normal\"],\n",
    "                      ordered=True)\n",
    "\n",
    "df.Phenotype2 = pd.Categorical(df.Phenotype2, \n",
    "                      categories=[\"Abnormal\",\"Abnormal & Unclear\",\"Normal\"],\n",
    "                      ordered=True)\n",
    "\n",
    "df.Phenotype3 = pd.Categorical(df.Phenotype3, \n",
    "                      categories=[\"Abnormal\",\"Normal\"],\n",
    "                      ordered=True)\n",
    "\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "df = df.dropna(subset=['Age'])\n",
    "\n",
    "fig = plt.figure(figsize=(3.5, 6))\n",
    "ax = fig.add_axes([0.25, 0.15, 0.70,0.80])\n",
    "\n",
    "ax = sns.violinplot(\n",
    "    data=df,\n",
    "    x='Phenotype',\n",
    "    y='Age',\n",
    "    #hue='Annotated',\n",
    "    split=True,\n",
    "    #inner=\"box\",\n",
    "    #common_norm = True,\n",
    "    saturation = 1,\n",
    "    inner_kws=dict(box_width=30, whis_width=2, color=\".8\"),\n",
    "    density_norm=\"count\",\n",
    "    palette=colors_UKBB\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(['IRD or Abnormal', 'Normal'])\n",
    "\n",
    "ax.set(xlabel=\"Phenotype\")\n",
    "ax.set_ylim(-10, 110)\n",
    "\n",
    "\n",
    "\n",
    "#sns.swarmplot(x ='set', y ='Age', data = df,color= \"white\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title('UKB Age Distribution')\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"UKBBAgeDist.png\", dpi = 300)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
